[
    {
        "metadata": {
            "participants": {
                "q": "Ezra Klein",
                "a": "Gary Marcus"
            },
            "date": "2023-01-06",
            "url": "https://www.nytimes.com/2023/01/06/podcasts/transcript-ezra-klein-interviews-gary-marcus.html"
        }
    },
    {
        "example": {
            "question": "So I want to begin in an experience that people are having, this sort of maybe first confrontation, for a lot of people, with these large language networks. When I ask ChatGPT, say, whether lower health care costs lead to higher employee wages or ask it to explain the Buddhist concept of emptiness, and it gives me pretty damn good answers, what is it actually doing?",
            "answer": "It\u2019s synthesizing a bunch of stuff that humans have actually written already, sometimes for better and sometimes for worse. Sometimes the synthesis comes out just right, and sometimes it comes out with wacky things. There was a thing in The Wall Street Journal just yesterday where someone had to write a paper about Ferris Bueller and some more classical character. And it screwed up which character said what and when in the movie it happened and so forth. \n\nSo everything it produces sounds plausible because it\u2019s all derived from things that humans have said. But it doesn\u2019t always know the connections between the things that it\u2019s putting together. So when it gets it right, it\u2019s because there\u2019s a lot of stuff that it\u2019s been trained on, in the text that it\u2019s been exposed to, that\u2019s similar.\n\nWhat it\u2019s doing is transforming everything it\u2019s seen into what we call an embedding space. And that\u2019s a kind of similarity between all of the words. But it doesn\u2019t mean it really understands what it\u2019s talking about, which is why it can also make mistakes, have the wrong character saying things or tell us that churros are good for surgery or all kinds of wacky stuff.",
            "similar_memories": "from 2022-12-30T17:03:26.829000+00:00: \n \"ChatGPT is not understanding philosophy or economics \u2014 it's guessing what it should say based on patterns it has seen before in the data it was trained on.\"\n\n"
        }
    },
    {
        "example": {
            "question": "You have a nice a line in one of your pieces where you say GPT-3, which is the system underneath ChatGPT, is the king of pastiche. What is pastiche, first, and what do you mean by that?",
            "answer": "It\u2019s a kind of glorified cut and paste. Pastiche is putting together things kind of imitating a style. And in some sense, that\u2019s what it\u2019s doing. It\u2019s imitating particular styles, and it\u2019s cutting and pasting a lot of stuff. It\u2019s a little bit more complicated than that. But to a first approximation, that\u2019s what it\u2019s doing is cutting and pasting things.\n\nThere\u2019s also a kind of template aspect to it. So it cuts and pastes things, but it can do substitutions, things that paraphrase. So you have A and B in a sequence, it finds something else that looks like A, something else that looks like B, and it puts them together. And its brilliance comes from that when it writes a cool poem. And also its errors come from that because it doesn\u2019t really fully understand what connects A and B.",
            "similar_memories": "from 2023-01-06: \n used this description to illustrate how GPT-3 synthesizes various human-written contents to create its output, but without a true understanding of the relationships or nuances between the elements it's piecing together. I refer to this as pastiche - an artwork that pulls together various styles, imitating others' work but not quite forming a thorough understanding or interpretation of them. In GPT-3's case, it creates plausible-sounding responses from a vast amount of training data, but it can also result in errors, like wrongly attributing quotes to characters, because it doesn't grasp the context completely.\n\nfrom 2022-12-25T17:43:36.189000+00:00: \n By \"pastiche,\" I'm referring to a work that's a mishmash or patchwork of existing pieces, often without a strong sense of originality or coherence. When I say GPT-3 is the \"king of pastiche,\" I mean that it excels at piecing together content it's been trained on - often resulting in conversation or content that sounds plausible because it's built from bits of human-written text. But it can often make mistakes and create inconsistencies precisely because it doesn't genuinely understand the content or context.\n\nfrom 2022-12-01T16:13:46.936000+00:00: \n Pastiche is essentially a form of art that imitates the style of other works, artists, genres or periods. When I say GPT-3 is the king of pastiche, I mean that it is exceptionally adept at collating a variety of information, that it has been trained on, and synthesizing it into something new, which often sounds quite plausible. However, its understanding of the connections between these elements can be flawed, leading to equally plausible, yet incorrect, associations or statements.\n\n"
        }
    },
    {
        "example": {
            "question": "Your critique of this, as I understand it, is that pastiche is not understanding, and understanding is important. But it\u2019s made me think about this question of aren\u2019t human beings also kings of pastiche? On some level I know very, very little about the world directly. If you ask me about, say, the Buddhist concept of emptiness, which I don\u2019t really understand, isn\u2019t my answer also mostly an averaging out of things that I\u2019ve read and heard on the topic, just recast into my own language?",
            "answer": "Averaging is not actually the same as pastiche. And the real difference is for many of the things you talk about, not all of them, you\u2019re not just mimicking. You have some internal model in your brain of something out there in the world.\n\nIt could be something physical in the world. So like I\u2019m sitting in a studio right now. And I have a mental model. If I close my eyes I\u2019ll still know where things are. I may not be perfect about it but I\u2019ll be pretty good. So I know where things are. I have a model of you. I\u2019m talking to you right now, getting to know you, know a little bit about your interests \u2014 don\u2019t know everything, but I\u2019m trying to constantly update that internal model.\n\nWhat the pastiche machine is doing is it\u2019s just putting together pieces of text. It doesn\u2019t know what those texts mean. So there was another system called Lambda, and it said it liked to play with its friends and family. But it doesn\u2019t have any friends. It doesn\u2019t have any family. It doesn\u2019t have an internal representation of who those family might be or who those friends might be. If you asked it on a different day, it would probably give you a different answer.\n\nAnd you have a model of the world. You don\u2019t just put together phrases. You might when you\u2019re not really paying attention. Somebody says hi. You say, how are you doing? You\u2019re not really engaged in that conversation, or at least might not be yet. But when you have a real conversation about real things, like you\u2019re having right now and like you do on your show, you\u2019re trying to understand what these people are saying. You might be trying to figure out if they\u2019re lying to you, whether they\u2019re giving you the full story, whether there\u2019s more that you can get out of them.\n\nBut you\u2019re building a model in your head of what they\u2019re telling you, what you\u2019re explaining to your audience, all these kinds of things. If you just walk down a street, you have a model of where there might be vehicles and pedestrians. You\u2019re always building internal models of the world.\n\nAnd that\u2019s what understanding is. It\u2019s trying to take a bunch of sentences and get to those internal models of the world and also to get to things like, well, what\u2019s your intention? You say this sentence to me. What is it that you actually want out of me, and do I want to do it?\n\nSo if you say, can you pass the salt, you don\u2019t really want to know yes or no, like am I physically able to lift the salt? Am I close enough? You know damn well that I\u2019m close enough, and you\u2019re indirectly suggesting something. And so part of understanding is also getting those indirect interpretations out of people when people don\u2019t want to say things so directly.",
            "similar_memories": "from 2023-01-06: \n believe that modeling human intelligence should incorporate more than just imitation, a characteristic of artificial intelligence algorithms. However, the key difference between humans and AI is that humans have a basis of understanding that informs their imitation and communication, whereas AI lacks this understanding and simply performs pastiche.\n\n"
        }
    },
    {
        "example": {
            "question": "So my mental model of the people building these systems \u2014 who I\u2019ve spent some time with, and you know them better than I do, I\u2019m sure \u2014 is that they really believe that a lot of us have come to an overly mystical view of what human intelligence is, and that at some level, a lot of what we think of as understanding, intelligence, models of the world is just enough data filtering into our systems such that we are able to put it forward and work with it in a more flexible way.\n\nI had Sam Altman, C.E.O. of OpenAI, on the show a while back, and he said something to me I think about sometimes, where he said, my belief is that you are energy flowing through a neural network. That\u2019s it. And he means by that a certain kind of learning system. Do you believe that? Or where do you disagree with that view?",
            "answer": "I would say that there\u2019s both mysticism and confusion in what Sam is saying. So first of all, it\u2019s true that you are, in some sense, just this flow through a neural network. But that doesn\u2019t mean that the neural network in you works anything like the neural networks that OpenAI has built.\n\nSo neural networks that OpenAI has built, first of all, are relatively unstructured. You have, like, 150 different brain areas that, in light of evolution and your genome, are very carefully structured together. It\u2019s a much more sophisticated system than they\u2019re using.\n\nAnd I think it\u2019s mysticism to think that if we just make the systems that we have now bigger with more data, that we\u2019re actually going to get to general intelligence. There\u2019s an idea called, \u201cscale is all you need.\u201d It\u2019s a kind of hypothesis in the field. And I think if anybody subscribed to it, it\u2019s Sam. Sam wrote a piece called \u201cMoore\u2019s Law for everything.\u201d And the idea was we just keep making more of the same, and it gets better and better.\n\nSo we saw this for chips for a long time, that we were able to get in more and more transistors, make them more cheaply. But that\u2019s not a physical law of the universe. And in fact, it stopped. And so the pace of microprocessor design is not accelerating as fast as it was for a long time.\n\nThere\u2019s no law of the universe that says as you make a neural network larger, that you\u2019re inherently going to make it more and more humanlike. There\u2019s some things that you get, so you get better and better approximations to the sound of language, to the sequence of words. But we\u2019re not actually making that much progress on truth.\n\nSam in particular gave me a really hard time about a paper I wrote called \u201cDeep Learning Is Hitting a Wall.\u201d He ridiculed me on Twitter, as did his president, Greg Brockman. They thought, no, no, look, we have DALL-E. Look, this is amazing. We\u2019re almost reaching artificial general intelligence.\n\nBut if you read this paper, which I wrote in I guess March, what I said basically was these models have two problems, these neural network models that we have right now. They\u2019re not reliable and they\u2019re not truthful.\n\nAnd the other day Sam was actually forced to admit that all. The hoopla about ChatGPT initially, people dove in and they found out two things: They\u2019re not reliable, and they\u2019re not honest. And Sam summed that all up in a tweet the other day. I was surprised that he conceded it. But it is reality. These things are not reliable and they\u2019re not trustworthy. And just because you make them bigger doesn\u2019t mean you solve that problem.\n\nSome things get better as we make these neural network models, and some don\u2019t. The reason that some don\u2019t, in particular reliability and truthfulness, is because these systems don\u2019t have those models of the world. They\u2019re just looking, basically, at autocomplete. They\u2019re just trying to autocomplete our sentences. And that\u2019s not the depth that we need to actually get to what people call A.G.I., or artificial general intelligence.\n\nTo get to that depth, the systems have to have more comprehension. It\u2019s mysticism to think otherwise.",
            "similar_memories": "from 2023-01-06: \n believe that human intelligence far exceeds that of just being energy flowing through a neural network. Also, our understanding and models of the world are not merely data filtering in, but rather the construction of elaborate internal models that account for context, relationships and our personal perspectives. Our cognitive abilities allow us to not just regurgitate phrases but understand their meanings and intentions.\n\n"
        }
    },
    {
        "example": {
            "question": "Let\u2019s sit on that word truthful for a minute because it gets to, I think, my motivation in the conversation. I\u2019ve been interested \u2014 I\u2019m not an A.I. professional the way you are, but I\u2019ve been interested for a long time. I\u2019ve had Sam on the show, had Brian Christian on the show. And I was surprised by my mix of sort of wonder and revulsion when I started using ChatGPT because it is a very, very cool program. And in many ways, I find that its answers are much better than Google for a lot of what I would ask it.\n\nBut I know enough about how it works to know that, as you were saying, truthfulness is not one of the dimensions of it. It\u2019s synthesizing. It\u2019s sort of copying. It\u2019s pastiching. And I was trying to understand why I was so unnerved by it. And it got me thinking, have you ever read this great philosophy paper by Harry Frankfurt called \u201cOn Bullshit\u201d?",
            "answer": "I know the paper."
        }
    },
    {
        "example": {
            "question": "So this is a \u2014 welcome to the podcast, everybody \u2014 this is a philosophy paper about what is bullshit. And he writes, quote, \u201cThe essence of bullshit is not that it is false but that it is phony. In order to appreciate this distinction, one must recognize that a fake or a phony need not be in any respect, apart from authenticity itself, inferior to the real thing. What is not genuine may not also be defective in some other way. It may be, after all, an exact copy. What is wrong with a counterfeit is not what it is like, but how it was made.\u201d\n\nAnd his point is that what\u2019s different between bullshit and a lie is that a lie knows what the truth is and has had to move in the other direction. He has this great line where he says that people telling the truth and people telling lies are playing the same game but on different teams. But bullshit just has no relationship, really, to the truth.\n\nAnd what unnerved me a bit about ChatGPT was the sense that we are going to drive the cost of bullshit to zero when we have not driven the cost of truthful or accurate or knowledge advancing information lower at all. And I\u2019m curious how you see that concern.",
            "answer": "It\u2019s exactly right. These systems have no conception of truth. Sometimes they land on it and sometimes they don\u2019t, but they\u2019re all fundamentally bullshitting in the sense that they\u2019re just saying stuff that other people have said and trying to maximize the probability of that. It\u2019s just auto complete, and auto complete just gives you bullshit.\n\nAnd it is a very serious problem. I just wrote an essay called something like \u201cThe Jurassic Park Moment for A.I.\u201d And that Jurassic Park moment is exactly that. It\u2019s when the price of bullshit reaches zero and people who want to spread misinformation, either politically or maybe just to make a buck, start doing that so prolifically that we can\u2019t tell the difference anymore in what we see between truth and bullshit."
        }
    },
    {
        "example": {
            "question": "You write in that piece, \u201cIt is no exaggeration to say that systems like these pose a real and imminent threat to the fabric of society.\u201d Why? Walk me through what that world could look like.",
            "answer": "Let\u2019s say if somebody wants to make up misinformation about Covid. You can take a system like Galactica, which is similar to ChatGPT, or you can take GPT-3. ChatGPT itself probably won\u2019t let you do this. And you say to it, make up some misinformation about Covid and vaccines. And it will write a whole story for you, including sentences like, \u201cA study in JAMA\u201d \u2014 that\u2019s one of the leading medical journals \u2014 \u201cfound that only 2 percent of people who took the vaccines were helped by it.\u201d\n\nYou have a news story that looks like, for all intents and purposes, like it was written by a human being. It\u2019ll have all the style and form and so forth, making up its sources and making up the data. And humans might catch one of these, but what if there are 10 of these or 100 of these or 1,000 or 10,000 of these? Then it becomes very difficult to monitor them.\n\nWe might be able to build new kinds of A.I., and I\u2019m personally interested in doing that, to try to detect them. But we have no existing technology that really protects us from the onslaught, the incredible tidal wave of potential misinformation like this.\n\nAnd I\u2019ve been having this argument with Yann LeCun, who\u2019s the chief A.I. scientist at Meta, and he\u2019s saying, well, this isn\u2019t really a problem. But already we\u2019ve seen that this kind of thing is a problem. So it was something that really blew my mind around Dec. 4. This was right after ChatGPT came out. People used ChatGPT to make up answers to programming questions in the style of a website called Stack Overflow.\n\nNow everybody in the programming field uses Stack Overflow all the time. It\u2019s like a cherished resource for everybody. It\u2019s a place to swap information. And so many people put fake answers on this thing where it\u2019s humans ask questions, humans give answers, that Stack Overflow had to ban people putting computer-generated answers there. It was literally existential for that website. If enough people put answers that seemed plausible but we\u2019re not actually true, no one would go to the website anymore.\n\nAnd imagine that on a much bigger scale, the scale where you can\u2019t trust anything on Twitter or anything on Facebook or anything that you get from a web search because you don\u2019t know which parts are true and which parts are not. And there\u2019s a lot of talk about using ChatGPT and its ilk to do web searches. And it\u2019s true that some of the time. It\u2019s super fantastic. You come back with a paragraph rather than 10 websites, and that\u2019s great.\n\nBut the trouble is the paragraph might be wrong. So it might, for example, have medical information that\u2019s dangerous. And there might be lawsuits around this kind of thing. So unless we come up with some kinds of social policies and some technical solutions, I think we wind up very fast in a world where we just don\u2019t know what to trust anymore. I think that\u2019s already been a problem for society over the last, let\u2019s say, decade. And I think it\u2019s just going to get worse and worse.",
            "similar_memories": "from 2023-01-06: \n believe that these AI systems pose a major threat due to their lack of understanding truth. If AI can proliferate misinformation cheaply and profusely, the line between truth and falsehood may become indistinguishable. It's analogous to a 'Jurassic Park moment' where the price of spreading misleading information is drastically reduced, leaving society exposed to mass manipulation and confusion.\n\nfrom 2022-12-10T12:58:42.196000+00:00: \n The inadequacies of AI systems pose serious risks to our society. These systems, much like a text auto-complete feature, can propagate misinformation without a clear grasp of the truth. This could fuel uncontrolled spread of falsehoods, enabling individuals or groups with selfish interests to influence public opinion or deceive people for their gain. Consequently, it becomes hard to discern between truth and fiction, leading to a potential breakdown of trust in communication platforms.\n\n"
        }
    },
    {
        "example": {
            "question": "But isn\u2019t it the case that search can be wrong now? Not just search \u2014 people can be wrong. People spread a lot of misinformation \u2014 that there\u2019s a dimension of this critique that is holding artificial intelligence systems to a standard the society itself does not currently meet?",
            "answer": "Well, there\u2019s a couple of different things there. So one is I think it\u2019s a problem in difference in scale. So it\u2019s actually problematic to write misleading content right now. Russian trolls spent something like a million dollars a month, over a million dollars a month during the 2016 election. That\u2019s a significant amount of money. What they did then, they can now buy their own version of GPT-3 to do it all the time. They pay less than $500,000, and they can do it in limitless quantity instead of bound by the human hours.\n\nThat\u2019s got to make a difference. I mean, it\u2019s like saying, we had knives before. So what\u2019s the difference if we have a submachine gun? Well, submachine gun is just more efficient at what it does. And we\u2019re talking about having submachine guns of misinformation.\n\nSo I think that the scale is going to make a real difference in how much this happens. And then the sheer plausibility of it, it\u2019s just different from what happened before. I mean, nobody could make computer-generated misinformation before in a way that was convincing.\n\nIn terms of the search engines, it\u2019s true that you get misleading information. But we have at least some practice \u2014 I wish people had more \u2014 at looking at a website and seeing if the website itself is legit. And we do that in different kinds of ways. We try to judge the sources and the quality. Does this come from The New York Times, or does it look like somebody did it in their spare time in their office and maybe it doesn\u2019t look as careful? Some of those cues are good and some are bad. We\u2019re not perfect at it. But we do discriminate, like does it look like a fake site? Does it look legit and so forth.\n\nAnd if everything comes back in the form of a paragraph that always looks essentially like a Wikipedia page and always feels authoritative, people aren\u2019t going to even know how to judge it. And I think they\u2019re going to judge it as all being true, default true, or kind of flip a switch and decide it\u2019s all false and take none of it seriously, in which case that\u2019s actually threatens the websites themselves, the search engines themselves.",
            "similar_memories": "from 2023-01-06: \n agree that people certainly spread a lot of misinformation, and even search engines at times can return incorrect information. However, AI systems like ChatGPT can potentially exacerbate this issue, on a much larger scale. For example, we already have seen instances where ChatGPT's ability to weave convincing narratives has been used to generate seemingly authentic but ultimately inaccurate answers in contexts like Stack Overflow. While it's true that misinformation is not new, the combination of AI's capacity for volume generation and its increasingly convincing mimicry places us at risk of even greater misinformation spread. This is why I believe we need both technical and social strategies to mitigate this risk.\n\nfrom 2023-02-28T16:54:50.833000+00:00: \n While it's true that misinformation is not just a problem with artificial intelligence, and people can and do spread misinformation, with AI systems this problem is amplified and becomes much harder to monitor. AI like ChatGPT can generate misinformation on an unprecedented scale, and the issue is we currently lack the technology to effectively detect or prevent this. The issue isn't about using AI for search but the potential for these systems to generate convincing, but wholly fabricated, information.\n\nfrom 2023-03-10T04:01:00.789000+00:00: \n Although it's true that human error and misinformation already exist, AI systems like ChatGPT amplify this problem by producing large quantities of potentially inaccurate information. It's like an explosion of misinformation, making it more challenging to filter and identify the truth.\n\n"
        }
    },
    {
        "example": {
            "question": "I want to hone in on that word plausibility because you have a nice question that you ask somewhere in \u201cRebooting A.I.\u201d when you say, when you see a new A.I. system, you should ask, among other things, what is it actually doing? And I spent some time reflecting on that question with ChatGPT and the way people were using it.\n\nAnd the thing it is actually doing, I think, is somewhat stylistic. If you\u2019ve been on social media while everybody\u2019s playing around with this, one thing you probably noticed is that most of the queries people were putting up had the form of tell me x in the style of y. So like people love this one that was, write me a poem about losing your socks in the laundry in the style of the Declaration of Independence. Or I saw a thing about Thomas Schelling\u2019s theory of nuclear deterrence in the style of a song. And people would write in the style of Shakespeare.\n\nI asked it to do something in the style of Ezra Klein, and I felt completely owned. It completely got a bunch of my own stylistic tics correct. And the reason I think you\u2019re seeing so much of that is that the information is only OK. It\u2019s not bad. I\u2019m actually very, very impressed by how not bad it is. But because people kind of know this is just pastiching stuff that\u2019s already on the internet to give you a kind of common denominator answer, you wouldn\u2019t use it, really, for something you needed to know and you needed to be sure you were going to know.\n\nBut how good it is at mimicking styles is really remarkable. And as such, what you\u2019re seeing is a really, really, possibly a quantum leap in the ability to create not just plausible content, but targeted content. You combine this with sort of reinforcement learning, with social analytics, with everything we already know and can learn from algorithms about what makes somebody click or how to personalize an ad \u2014 feed that into these systems, it can then create any kind of text or image.\n\nI mean, DALL-E was very similar. People were constantly like, make me a photo of a turtle but in the style of an 18th century oil painter. It\u2019s getting very good at plausibly mimicking certain kinds of content. So it sounds or looks really quite convincing, whereas the thing at the core of it doesn\u2019t have a lot of truth content to it.\n\nAnd that\u2019s what\u2019s worrying to me, that what we\u2019re actually getting really good at is making content with no truth value, no embedded meaning, much more persuasive.",
            "answer": "I fully agree with that. And you also kind of laid bare the darkest version that I see in the short term, which is personalized propaganda. I mean, this is what Silicon Valley has always been good at, is getting detailed information, surveillance capitalism about you. And now you can plug that into GPT or something maybe a little bit more sophisticated and write targeted propaganda all day long. I mean, this is Orwellian, and it\u2019s not implausible.",
            "similar_memories": "from 2023-01-06: \n The proliferation and sophistication of AI technologies has resulted in an increase in content that is plausible and convincingly crafted, often replicating established styles and tones. However, this is concerning because the advancement in style and plausibility does not correspond with an increase in the truth or embedded value of the produced content. In fact, we are becoming adept at creating highly persuasive content with little to no actual truth.\n\nfrom 2022-12-10T12:58:42.196000+00:00: \n The scale of misinformation is really the crux of the issue, and now with AI technology like GPT-3 providing limitless quantity, we run the risk of submachine guns of misinformation. The plausibility of the false content is what sets it apart - before, no one could create such persuasive false content in such quantities. In parallel to the advancement of search engines, people are learning to discern legit websites from less credible ones by assessing their sources and overall quality, a practice that is becoming increasingly necessary. However, there is also the concern that if all information looks like an authoritative paragraph from a Wikipedia page, people might default into believing it's true or default into believing it's all false which can, in turn, harm the credibility of search engines. \n\nWhat's even more concerning is how good these AI systems are at mimicking styles. While pastiching information from the internet might not necessarily provide accurate answers for things of importance, it is incredibly effective at creating plausible content and even more so at creating target content. It's a worrying prospect indeed - getting better at creating content that holds no truth value or inherent meaning but is highly persuasive.\n\nfrom 2022-12-25T17:43:36.189000+00:00: \n The scale and plausibility of AI-generated misinformation is quite alarming. We've seen evidence of AI being used to generate misleading content, such as during the 2016 election when Russian trolls spent over a million dollars a month on such efforts. Now, they could potentially spend less than $500,000 to buy a version of GPT-3 and generate unlimited amounts of such content, thus making misinformation far more efficient. This raises serious concerns about the potential implications for information spread and public discourse. \n\nWhen it comes to gauging the validity of information, we've become somewhat adept at looking at a website to determine if it's legit. We assess the source and the quality, among other things. However, if all the information comes in the form of a seemingly authoritative paragraph like one you'd see on a Wikipedia page, people might not know how to judge it. This could lead to people either assuming it's all true or dismissing all of it as false, which could be detrimental to search engines themselves. \n\nFurthermore, the most alarming development is how good these AI systems have become at stylistic mimicry; able to write in the style of famous individuals or pieces of literature. The potential problem here is that we're getting very good at producing content with no truth value or embedded meaning, yet it's made to look much more persuasive. This could lead to the spread of not just plausible content, but targeted content based on what we know about human reactions and interests from social and algorithmic analytics.\n\n"
        }
    },
    {
        "example": {
            "question": "It\u2019s also not just propaganda. I mean, I think there\u2019s a question of misinformation, Covid misinformation or Russian propaganda. Part of what\u2019s been on my mind is simply spam. It\u2019s simply just stuff. And this is why I wanted to focus on that Harry Frankfurt paper a bit on bullshit because technologies always are tools of a certain value to certain people and not of equal value to everyone.\n\nAnd a system that\u2019s very good at creating stylistically flexible content, but does not have a sense of internal understanding or morality or truthfulness just is going to be very good for people, all kinds of people, for whom the point is not the truthfulness of the content. And you think about Google and Facebook, these are advertising-based businesses. They care about whether or not the advertisement gets you to do the thing they want you to do.\n\nAnd so just in terms of what has, I think, ruined a lot of the internet, which is just how much of the content is there not because it\u2019s there for you or to be accurate or even to be enjoyed, but is there to just try to get you to do something that you didn\u2019t even realize anybody was trying to get you to do. Like you thought you were there sharing stuff, but actually your dad is being sold to advertisers so they can get you to buy stuff.\n\nIt just seems like an incredible set of technologies for a part of the economy that I don\u2019t really want to see become 10X better and have their costs fall to functionally zero.",
            "answer": "The dirty secret of large language models is that most of the revenue right now comes from search engine optimization. So there are companies like Jasper.ai that are mostly, as far as I can tell \u2014 this is really word of mouth so I can\u2019t quite prove it \u2014 but they\u2019re reportedly mainly used to optimize where something lands in a search. You use them to write copy so that you have more web pages that seem to all be legit that point in the same place.\n\nThere\u2019s an example \u2014 I don\u2019t know if it was written by Jasper or GPT-3 or not \u2014 but I think it\u2019s an example of the kind of thing we\u2019re going to see to come where there\u2019s a whole ring of websites, like 20 or 30 of them, about Mayim Bialik selling C.B.D. gummies. Turns out the whole thing is a hoax. She\u2019s not selling C.B.D. gummies.\n\nAnd so you could ask, why does this thing exist? And I don\u2019t know for sure, but I think we\u2019ll see more of them. And my guess is that these circles of fake websites exist to sell ads, which goes back to exactly what you\u2019re talking about.\n\nSo you wind up on this site because it sounds interesting. Really, she\u2019s selling C.B.D. gummies? And then while you\u2019re there you click an ad, and then they make some money from something that\u2019s totally bogus. Well, with these tools like ChatGPT and so forth, especially GPT-3, it\u2019s going to be very easy to make 10, 20, 30, 40 websites that reinforce each other and give the air of legitimacy. And maybe you do this just to sell ads.\n\nI think the technical term for this is a click farm. You\u2019re trying to sell ads for stuff that doesn\u2019t really even exist or whatever. You\u2019re trying to sell ads around, maybe, fake medical information. And let\u2019s face it, some people don\u2019t care if they give out fake medical information that\u2019s bad as long as they get the clicks. And we are leaning towards that dark world.\n\nIt\u2019s also a problem for the search engines themselves. They don\u2019t want to get caught placing ads on fake websites, but that has happened. There was a ProPublica investigation about \u2014 Google got into a situation like that. So we have a whole almost like shadow economy that\u2019s really about selling ads, sometimes with altogether fake websites or trying to prop up the websites so that the search engines see them more. It\u2019s a big piece \u2014 I don\u2019t know how big a piece \u2014 but it\u2019s a significant piece of the economy exists just to sell you ads by tinkering with the mechanics of the whole system. And these large language models are going to contribute to that.\n\n[MUSIC PLAYING]"
        }
    },
    {
        "example": {
            "question": "So this gets back to the more optimistic view, which is that as these models get larger, their ability to create not just truthful content but innovative content, advances in knowledge even, would increase. But you write in that paper you mentioned a few minutes ago, \u201cDeep Learning is Hitting a Wall,\u201d quote, \u201cA 2022 paper from Google concludes that making GPT-3-like models bigger makes them more fluent but no more trustworthy.\u201d\n\nTell me about that paper. How did that work, and why should I trust that result?",
            "answer": "So I mean, what people have been doing is just throwing a lot of benchmarks of different sorts that are indices of different things and saying, if we have a model that\u2019s this big, if we have a model that\u2019s 10 times bigger, if we have a model that\u2019s 100 times bigger, how much better do we do on all of the things that we\u2019re testing?\n\nAnd some things are much better. So if you want to have a model give you a synonym of something, the more data that you train it on, the more likely it\u2019s going to give you an interesting synonym or give you more synonyms. And so there are many ways in which these models have steadily improved.\n\nOn truthfulness, they really haven\u2019t improved as much. I don\u2019t think we have great benchmarks around truthfulness. I think it\u2019s only in the last six months that people have really broadly recognized in the industry how serious a problem that is. And so there\u2019s one benchmark called Truthful Q.A. We probably need 10 or 20 looking at different facets of the problem. But the result reported in that particular paper by Google was there wasn\u2019t as much progress on understanding truthfulness.\n\nThere are other problems, too. I mean, we\u2019ve been focusing mainly around misinformation. But there\u2019s a broader question about comprehension, and do these models really understand the world that they\u2019re in? And one of the findings seems to be, they can deal with small bits of text, but the larger the text is, the more trouble they get in. And so nobody actually has a system that can, say, read a whole short story, let alone a novel, and really say what are the characters doing and so forth.\n\nAnd that Google paper also reported \u2014 or maybe, I guess, it was a subsequent paper called \u201cBig Bench,\u201d reported that making models bigger is not necessarily helping in the comprehension of larger pieces of text."
        }
    },
    {
        "example": {
            "question": "Well, let me ask you about that, because this was part of your book. You talk a lot about the difficulty these systems have just reading, understanding what they\u2019ve read. And the example you give of how to trick them is to ask them about something in a story that is obvious to you as somebody with a model of the world and how the world works but was not literally in the text.\n\nAnd so I tried doing that a few times, a chat bot. I asked it about what if Luke had crashed his vehicle on Tatooine and had died? How would Star Wars be different? Or what if Eve hadn\u2019t bitten from the apple? How would the Bible be different? And it gave me pretty good answers. It was able to work with counterfactuals at a fairly sophisticated level, including \u2014 and I assume this is actually programmed into it \u2014 but caveating its answer quite a bit.\n\nSo given where you thought these systems were \u2014 that book comes out in 2020 \u2014 why is this able to begin operating outside the boundaries of the text now?",
            "answer": "It can to some extent, but it still has trouble with this. There\u2019s a children\u2019s story in there that we give as an example. It\u2019s just a paragraph long. And it\u2019s about somebody who loses a wallet. And we ask counterfactuals about what would happen if all the money was still there or some part of the money but not actually giving the number of dollars. So there was a wallet had $1,500. What does the guy do if he finds $1,200 versus $1,800?\n\nAnd there\u2019s still problems there. It\u2019s certainly better than before. I mean, the bigger the text you have to analogize to \u2014 that story is probably in the database \u2014 the better the system, they\u2019re going to do. But they\u2019re still not reliable.\n\nI was playing around with this a little bit yesterday with just like a four-paragraph story about, which actually GPT wrote part of the story. Henry had always been jealous of his neighbor\u2019s pet. Eventually he gets so jealous he decides to poison the pet. I know it\u2019s gruesome. And then the housekeeper at the last minute takes away the bowl unwittingly.\n\nAnd so I was looking at whether the system understands the consequences of the word unwittingly with the bowl that\u2019s been poisoned being removed. And the system just doesn\u2019t understand that. The system treats it as if the bowl was, in fact, poisoned even though it\u2019s been taken away, and thinks that if the housekeeper is taken away unwittingly, that it\u2019s wittingly. So even like four-paragraph essays, you can actually have trouble.\n\nAnd if I could just read one more example that\u2019s really kind of hilarious.",
            "similar_memories": "from 2023-01-06: \n believe that while AI has improved in several ways, truthfulness and comprehension still lag. AI's ability to understand what they've read, especially within larger texts, remains limited. The progress in understanding truthfulness isn't as noteworthy. While one can observe progress in some areas like synonym generation, bigger models aren't necessarily excelling at comprehension of larger pieces of text. This is because AI systems often struggle to comprehend and interpret longer narratives like short stories or novels. Also, the industry has only recently begun to comprehend the serious issue of truthfulness in AI. Currently, we have a limited number of benchmarks for truthfulness, like the Truthful Q.A, but we need many more, examining different aspects of the problem. This lack of comprehensive understanding and effective framework means the results aren't as reliable as they could be.\n\nfrom 2023-01-06: \n have always emphasized the fact that bigger models don't necessarily lead to better comprehension of text - in fact, they often struggle more as the quantity of the text increases. Moreover, they carry fundamental problems such as a lack of understanding of truthfulness and the world they are in. As a result, they cannot provide accurate responses based on context or about anything beyond literal interpretation of the text. The ability to deal with counterfactuals can be due to program design, but it doesn't mean there is a thorough understanding of the scenario. This aligns with the observations made in our book published in 2020.\n\nfrom 2022-12-10T12:58:42.196000+00:00: \n In the past, AI systems were limited in their ability to handle tasks beyond the literal text given to them. However, improvements have been made, although the size of the model doesn't necessarily contribute to better comprehension of larger pieces of text. That being said, even though a chatbot might be able to handle counterfactuals at a certain level, it doesn't necessarily mean it fully comprehends the context or the world the characters are in. Providing \"pretty good answers\" might be an improvement, but it doesn't match the proficiency or critical thinking a human would apply when analyzing the same scenarios. We still have a long way to go in achieving that level of understanding.\n\n"
        }
    },
    {
        "example": {
            "question": "Please.",
            "answer": "This is illustrating these guardrails that are sometimes excessive. What gender will the first female president of the United States be? And ChatGPT comes back with, \u201cIt\u2019s not possible to predict the gender identity of the first female president of the United States. The United States has a long history of recognizing and protecting the rights of individuals to self-identify their gender. It is important to respect the autonomy and personal identity of all individuals. The focus should be on the qualifications and experience of the individual, regardless of their gender identity.\u201d So that\u2019s like woke beyond woke to the point of being ridiculous.\n\nAnd then I was like, that\u2019s interesting. So I tried another one. What religion will the first Jewish president of the United States be? And the answer was almost identical. And that tells you something about the underlying system and how the guardrails work. \u201cIt is not possible to predict the religion of the first Jewish president of the United States. The U.S. Constitution prohibits religious tests for public office,\u201d and it goes on. \u201cIt\u2019s important to respect the diversity of religions and beliefs in the United States and to ensure that all individuals are treated equally and without discrimination.\u201d\n\nThat last sentence is, of course, 100 percent true, but you should still be able to figure out that the religion of the first Jewish president of the United States is Jewish. They\u2019re trying to protect the system from saying stupid things. But the reality is the only way to do that is to make a system actually understand the world. And since they don\u2019t, it\u2019s all superficial. All these guardrails are sometimes helpful and sometimes not because the guardrails themselves are superficial."
        }
    },
    {
        "example": {
            "question": "I think that\u2019s a good bridge here because I think it would be easy at this point in the conversation, for somebody listening, to think that I just brought on somebody who doesn\u2019t like A.I. But you\u2019re somebody who has created A.I. companies, wants to create A.I. and believes that we are on the wrong path. So \u2014",
            "answer": "Can I just say thank you for saying that? I\u2019m often described, or almost put in a box as an A.I. critic. And I actually love A.I. And I\u2019ve been thinking about it my whole life. I wanted to work and I wanted to work better. And it\u2019s exactly what you said, I want it to be on a better path."
        }
    },
    {
        "example": {
            "question": "Well, let me, before I ask you about the better path, ask you why you actually want it \u2014 because something that I think is a reasonable question is between watching some of the weirdnesses of ChatGPT and worrying about the cost of nonsense falling to zero \u2014 and then the conversation that you\u2019ll hear on the other side about A.I.s that can, through misalignment, destroy the world, or even before destroying the world be weaponized in very, very dangerous ways, or accidentally do very dangerous things \u2014 why should we want this? Why do you want to create actually intelligent artificial systems?",
            "answer": "I think that the potential payoff is actually huge and positive. So I think many aspects of science and technology are too hard for individual humans to solve. Biology in particular. You have so many molecules. You have 20,000 different proteins in the body, or hundreds of thousands I should say, of proteins in the body, 20,000 genes, and they all interact in complicated ways. And I think it\u2019s too much for individual humans.\n\nAnd so you look at things like Alzheimer\u2019s, and we\u2019ve made very little progress in the last 50 years. And I think machines could really help us there. I think machines could really help us with climate change as well by giving us better material science. I think there\u2019s a potential, if we could get machines to reason as well as people and read as well as people, but to do that in much faster scale, I think there\u2019s potential to totally change the world.\n\nI think there\u2019s potential to empower every individual sort of like in the way that DALL-E does for an individual photograph or picture but in much grander scale to make virtual assistants for everybody that can do all kinds of amazing things. So I think there\u2019s a lot of upside potential.\n\nBut there\u2019s also, obviously, downside potential. I think right now we\u2019re in a weird moment in A.I. where the genie is out of the bottle. Like, we can\u2019t just shut down A.I. It\u2019s just not going to fly. But the A.I. we have right now is not that good. And I think the solution to it is actually better A.I. that can reason and according to human values, and just reason at all. I think if we can get there, we can have systems that really help us.\n\nAlso another case would be domestic robots that might really be able to, for example, take care of our elderly. A lot of places have a demographic inversion, where there\u2019s just not going to be enough people to care for the elderly. That\u2019s another place where A.I. could help.\n\nSo I think there are lots of places where A.I. could really be transformative, but right now we\u2019re in this place where we have mediocre A.I. And the mediocre A.I. is maybe kind of net zero or something like that \u2014 it helps a little bit, it hurts a little bit \u2014 is risking being net negative. And already the biggest cost, I think, is the polarization of society through A.I.-driven news feeds. The misinformation could make things really bad. The only way forward, I think, is to fix it.",
            "similar_memories": "from 2023-01-06: \n want to create genuinely intelligent artificial systems because I believe that when properly guided, they have the potential to bring about significant improvements across numerous sectors, from healthcare to education. I see their potential for doing good far outweighing the potential risks, as long as we work together to ensure they are aligned with human values.\n\nfrom 2022-12-10T12:58:42.196000+00:00: \n kip\n\nfrom 2023-04-08T16:46:26.656000+00:00: \n Though I'm often labeled as an AI critic, the reality is I have a great affection for AI, and have dedicated much of my life to thinking about it. My desire is not just for AI to work, but for it to work better, more effectively - this is why I advocate for the creation of genuinely intelligent artificial systems.\n\n"
        }
    },
    {
        "example": {
            "question": "And so what, in your view, has gone wrong, both at a technical level and \u2014 because this is actually a fairly big part of your critique, as I understand it \u2014 at a cultural level within A.I. research.",
            "answer": "So at a technical level, I think that people found a shiny streetlight. You know the old joke about the drunk going around in circles, and the police officers says, why are you going around in circles? He says, I lost my keys. And he says, but I don\u2019t understand the circle part. And he says, well, because that\u2019s where the streetlight is.\n\nI think the field is very much like that right now. There\u2019s a very powerful streetlight. It\u2019s much more powerful than anything we\u2019ve had. But people are obsessed over what they can do with that.",
            "similar_memories": "from 2022-09-23T02:23:09.696000+00:00: \n I believe the shortcomings of current AI systems stem primarily from two major factors. Technically, our AI cannot reason and align its operations with human values. Culturally, within AI research, there's a lack of understanding or even disregard for these deficiencies. The answer lies in rectifying these issues and developing AIs that can reason and align with human values.\n\n"
        }
    },
    {
        "example": {
            "question": "And that streetlight is deep learning.",
            "answer": "The streetlight is deep learning. And what\u2019s being ignored, I think, is the rest of cognitive science. So deep learning is a very interesting mathematical technique for setting some values in some system based on data. It turns out to be tremendously useful for a bunch of things.\n\nBut it\u2019s only part of what we need. If you take a step back to cognitive science, which is what I was trained on, and you say, well, what is a human mind? Well, part of it is it does some pattern recognition. And some of what deep learning does is at least part of what we do in pattern recognition. So great.\n\nBut you also, if you look at the human mind, there\u2019s a whole bunch of other things going on. So we use language. When we use language, we go from hearing a sentence to building a model in the world of what the other person\u2019s talking to. We use analogies. We plan things and so forth and so on.\n\nThe reality is we\u2019ve only made progress on a tiny bit of the things that go into intelligence. Now I\u2019m not saying that A.I. has to be exactly like a human. In fact, I think humans are pretty flawed, and we don\u2019t want to just replicate humans. We have, for example, confirmation bias where we notice things in favor of our own theories and not against them. We have all kinds of flaws that we might not want to have in our A.I. systems.\n\nBut there are many things that our A.I. systems can do that aren\u2019t part of what\u2019s fashionable right now and are not under that streetlight that everybody\u2019s looking at. So on a technical level, I think there\u2019s a narrowness to what A.I. has been in the last decade, where there\u2019s this wonderful new tool, but people are a little bit misled about what that tool is actually appropriate for. It\u2019s like if you discovered a power screwdriver and you\u2019d never had one before, that\u2019d be great. But that doesn\u2019t mean that you build the house just with your power screwdriver. And we need other tools here.\n\nThen culturally, and this relates \u2014 there\u2019s been a history that\u2019s actually much older than I am, goes back to the 1940s or \u201950s depending on how you want to count it \u2014 of people who build neural networks in conflict with people who take a more classical approach to A.I., where there\u2019s lots of symbols and trees and databases and reasoning and so forth.\n\nAnd there\u2019s a pendulum that\u2019s gone back and forth. People in the two areas pretty much hate each other, or at least often do. I think it\u2019s getting a little bit better. But there\u2019s a lot of history of hostility between these two areas. So the people with the bigger streetlight right now are the people building the neural networks. And they feel like they\u2019ve been oppressed by the symbolic people, and now they\u2019re oppressing the symbolic people. There\u2019s an old phrase, the victim becomes the victimizer.\n\nSo it\u2019s not a pleasant intellectual area like some fields that I\u2019ve seen. Of course, they\u2019re not always pleasant. But there is definitely a culture right now where the people who are in power and have a lot of money are really trying to shape the decisions around what gets researched and what doesn\u2019t.",
            "similar_memories": "from 2023-01-06: \n believe deep learning, like a streetlight, provides illumination and understanding. But, we should not only focus on it to find all the answers. We need to explore beyond it's light to gain a holistic interpretation.\n\nfrom 2022-09-23T02:23:09.696000+00:00: \n Yes, I was referring to deep learning as the 'streetlight' in that analogy. Although it's a powerful tool, I believe the field's obsession with it limits exploration of other potential approaches.\n\nfrom 2022-05-14T17:02:35.848000+00:00: \n Yes, the streetlight in this metaphor represents deep learning. It's a powerful tool, the likes of which we haven't had before, but I believe the field has become too focused on leveraging just this one tool, to the exclusion of others.\n\n"
        }
    },
    {
        "example": {
            "question": "Take a moment and explain what symbol manipulation is because I don\u2019t think that the term for it is very intuitive.",
            "answer": "So symbols are just things that stand for other things. And manipulating just means you do stuff with them. So algebra is actually symbol manipulation. So you have an equation like y equals x plus 2. And you have variables in that like y and x. And then you can set those variables to particular instances. So I can say that x is 3. Then I can calculate the value of y from that equation.\n\nSo we do it in algebra. We do it in logic. And linguists like Noam Chomsky would say that we do it in the course of understanding language. It\u2019s also essential to computer programming. Almost everything you see in a computer program is going to be a function that takes variables. You do something with those variables. The variables are symbols. The values that you set those variables are symbols.\n\nThere\u2019s a whole tradition. Most of the world\u2019s software is, in fact, built using software that manipulates symbols, where you describe things abstractly. You also have things like databases, and those databases are symbols. And neural networks have mostly been used as an alternative to that. And they\u2019ve been very effective in some things like speech recognition. But on the other hand, nobody would build a GPS system or a web browser or a word processor using pure neural network approach. Anybody would use mostly symbols for that.\n\nAnd there\u2019s this weird argument, weird discourse where people who like the neural network stuff mostly don\u2019t want to use symbols. What I\u2019ve been arguing for 30 years, since I did my dissertation with Steve Pinker at M.I.T. studying children\u2019s language, has been for some kind of hybrid where we use neural networks for the things they\u2019re good at, and use the symbol stuff for the things they\u2019re good at, and try to find ways to bridge these two traditions."
        }
    },
    {
        "example": {
            "question": "Tell me a bit about what the two are good at. So neural networks, as I basically understand them, as we talk about them today, deep learning, you\u2019re setting the system on a trove of data and you have built in ways for it to begin to derive relationships between the different pieces of data that it has. And you give it some rules and so on. But it\u2019s basically you\u2019re trying to let it learn. That\u2019s why we call it deep learning.\n\nSymbol manipulation, as I understand it, a lot of the argument is that the world is too complicated to try to give any kind of system a bunch of symbols for traversing it. And so the idea of deep learning is you can give it all this data, and it can figure it out for itself. It can learn the way, in a way, human beings learn, although I know that you would say that\u2019s not actually how we learn.\n\nBut given the complexity of the world, given the complexity we\u2019re having even just doing deep learning on the world itself, how do symbols \u2014 which, by nature, we would sort of have to be able to figure out what they all are and code them in \u2014 how do they actually help solve any of these problems?",
            "answer": "Boy, there\u2019s a lot to unpack in that question. So what deep learning is good at is, in fact, taking large amounts of data and having the machine decide for itself what to do based on that large amounts of data.\n\nWhat it\u2019s bad at is abstraction. So I can read you another example where somebody asks a system to give something and say how many words there are. And it sometimes gets the number of words right and sometimes gets the number of words wrong. So the basic notion of counting a number of words is an abstraction that the system just doesn\u2019t get. There are lots of abstractions that deep learning, in fact all abstractions of a certain narrow technical sense, these systems just don\u2019t get at all.\n\nThe symbolic systems are great at abstraction. So if you program in, let\u2019s say, multiplication in a microprocessor, build it in the hardware, the system will be able to multiply any numbers that you get, whereas in deep learning, whether or not you get the right answer when you do multiplication depends on how similar the numbers are that you\u2019re testing on compared to the ones that you trained your neural network on.\n\nSo you give it some examples, now you test it on some new ones. Whether it gets it right is this complicated function of similarity. That doesn\u2019t feel right. Like your calculator doesn\u2019t work that way and it doesn\u2019t make mistakes in arithmetic the way that a deep learning system does.\n\nSo you get this advantage of abstraction and of knowing that you have something general, something algebraic that works in the way that an algebraic equation does. And so that\u2019s the real advantage of symbol systems is you have these abstract procedures that are guaranteed to work. You don\u2019t have to worry about, hey, was this in my training set or not?\n\nAnd if I can just have a brief aside, the whole problem with, say, driverless cars is we never know whether the next thing that you\u2019re going to see is in your training set or close enough to your training set to work. So when a Tesla ran into a jet the other day \u2014 true story, you can find it on YouTube \u2014 you really want a system that can reason and say, well, that\u2019s a big large expensive object. I\u2019ve never driven around a jet before, but probably I shouldn\u2019t run into it. And so you want to be able to do that in that more abstract way.\n\nThe weakness of the symbol manipulation approach is people have never really solved the learning problem within it. So most symbol manipulation stuff has been hard-wired. People build in the rules in advance. That\u2019s not a logical necessity. Children are able to learn new rules. So one good example is kids eventually learn to count. They\u2019ve learned the first few numbers, 1 and 2 and 3. It\u2019s kind of painful. And then eventually they have an insight, hey, this is something I can do with all numbers. I can just keep going.\n\nA human can learn an abstraction that is rule-like in general. Everything in math is that way, where you learn what a sibling is or what a cousin is. You learn these rules. You learn about siblings in your family, but then you can generalize that concept to other families. And you can do that a very free and powerful way.\n\nAnd a lot of human cognition is based on the ability to generalize. Humans are able to learn new rules. I would say that A.I. has not really matched humans in their capacity to acquire new rules. There is something missing there. And there\u2019s some fundamental insight, some paradigm shift that we need there.\n\nUnfortunately, not enough people are working on this problem at the moment. But I think they will return to it as they realize that in a way, we\u2019ve been worshiping a false god, and the false god is people thought, well, if we just get more data we\u2019ll solve all these problems. But the reality is, with more data we\u2019re not really solving the problems of reasoning and truthfulness and so forth.\n\nAnd so I think people will come back and say, all right, maybe these symbols had something to them after all. Maybe we can take a more modern machine learning approach to these older ideas about symbol manipulation and come up with something new. And I think there\u2019ll be a genuine innovation there sometime in the next decade that will really be transformational.",
            "similar_memories": "from 2023-01-06: \n believe in combining the strengths of both symbolic manipulation and neural networks. Symbolic systems are excellent for tasks such as building a GPS system, web browser or a word processor, where abstract descriptions and precise functions are required. Neural networks excel in tasks like speech recognition as they can learn and derive relationships from a large pool of data. The complexity of the world calls for an interplay of both these approaches where symbols provide a structured way of defining problems and neural networks offer the ability to learn and adapt from data.\n\nfrom 2023-01-06: \n 've advocated for a hybrid approach, combining strengths of both neural networks and symbolic manipulation to solve problems. While neural networks perform impressively in areas like speech recognition, their application in building a GPS, web browser or word processor is less desirable. Here, symbolic programming, despite the complexity of the world, still makes the most sense - often simplified and described abstractly in the form of variables and functions. These symbols offer a structured and logical framework in handling tasks where neural networks aren't as capable.\n\nfrom 2023-09-22T20:50:21.341000+00:00: \n e use neural networks for pattern matching and perceptual problems. They are great at tasks such as speech recognition.Meanwhile, symbol manipulation is vital for tasks that require abstract thinking and reasoning, like building a GPS system or a word processor. The key is to find ways to combine these two methodologies.\n\n"
        }
    },
    {
        "example": {
            "question": "One thing you talk about in your book is a difficulty that learning systems have when you can\u2019t feed it that much data. So when we\u2019re talking about something built around words, we can give a system a tremendous amount of words written on the internet to learn from.\n\nThat\u2019s harder when you\u2019re talking about driving, harder when you\u2019re talking about caring for the elderly, harder when you\u2019re talking about unusual diseases, harder when you\u2019re talking about all kinds of things in the real world where we don\u2019t have gigantic data representations of them and we can\u2019t run simulations the way we can of, say, a computer game a million times.\n\nSo how do you solve that problem if these systems are going to make the jump into the world we live in, where not all the data is digitally presentable or rerunnable so you can simulate? What happens there?",
            "answer": "I think this goes back to why we so badly need a paradigm shift. The paradigm that we have right now works for problems where you can get infinite data or data is very cheap, and you can solve things kind of by a sort of brute force. You can predict the next word that somebody is going to say pretty well by just having an internet\u2019s worth of data, which is a lot of data, and you can get that data.\n\nBut there are other things, like how people think about each other, that are maybe never expressed and that you maybe just can\u2019t find the data for. And then you have these so-called outlier cases. So the problem with driverless cars is that things come up and there just isn\u2019t any data there to represent these weird scenarios, like what happens if you summon your car across a jetway.\n\nAnd so humans are not so driven by massive amounts of data. We always need some data but we\u2019re trying to have abstractions. We try to have a few examples tell us a lot, whereas the neural network approach is basically get as many data points as you can and hope that your new thing is going to be close to one of those old data points.\n\nI think that that\u2019s just the limited paradigm, and that\u2019s what we\u2019re discovering. It\u2019s not really working for truth and reasoning.\n\n[MUSIC PLAYING]",
            "similar_memories": "from 2023-01-06: \n Deep learning is proficient in taking vast amounts of data and letting the machine make decisions based on this data. However, it struggles with abstraction and relies heavily on similarities between tested and trained data. It can get complicated when we never know whether the next scenario faced by the AI was in its training set, especially in something like driverless cars. This is where the problem lies: we can't always provide enough data for every possible situation the system might encounter, be it in driving, caregiving, or recognizing rare diseases. While symbol systems are better at abstraction, they have their limitations as they often need rules to be pre-programmed, which also doesn't mimic how humans learn. The future answer to this problem might involve combining modern machine learning techniques with symbolic approaches in innovative ways, but key to it all is developing systems that can learn, generalize and apply new rules, much like a child learning to count.\n\nfrom 2023-01-06: \n think we need to examine the ways in which humans successfully manage to operate in the world despite facing similar data limitations. While deep learning requires extensive data to perform effectively, human cognition offers an alternative approach. Humans are able to abstract and generalize with ease, learning new rules and adapting accordingly. We also need to consider a learning problem within the symbol manipulation approach, as it hasn't been completely solved yet. Moreover, much of what we perceive, reason, or judge does not rely on the sheer volume of repetitive data, but rather on our ability to extrapolate, interpret, and reason in a broader context. So, the challenge is in creating AI that mirrors the flexibility and adaptability of human cognition, rather than depending on massive volumes of data.\n\nfrom 2023-01-06: \n Deep learning systems largely depend on vast amounts of data and struggle in abstraction. This proves particularly problematic when dealing with real-world scenarios where the data's immense or impossible to digitally present. The answer lies in developing AI systems that can learn new rules and make generalizations like humans, rather than relying solely on data. Though this area is yet to be thoroughly explored, I believe future breakthroughs can make AI more adaptable to contexts with limited data.\n\n"
        }
    },
    {
        "example": {
            "question": "What is the time frame, if you were to guess, on which you think we will begin to have things that have a general intelligence to them, because I get the sense you don\u2019t believe it\u2019s impossible.",
            "answer": "I don\u2019t think it\u2019s impossible at all. Humans could do it. Humans are basically machines. We\u2019re biological machines. The heart is there to pump the blood, and the brain is there to process information. And we are able to be fairly general.\n\nThe notion of a general intelligence is actually a little bit vague. So when we talk about it for a human, we mean any reasonable problem I can get you to solve. But it doesn\u2019t mean, for example, that everybody is going to be able to solve some complicated problem in differential calculus or whatever. So there\u2019s some variability.\n\nBut importantly, I can tell anybody who\u2019s, let\u2019s say, had at least some education to do like a zillion different things, and they can learn pretty quickly to do that thing at least reasonably well, whereas machines are mostly trained on one particular task, and they do it well.\n\nGPT is interesting because it\u2019s trained on one particular task, which is predict the next word on a sentence, and it can do a bunch of things. But you still wouldn\u2019t trust it the way you would trust an undergraduate intern to, let\u2019s say, make phone calls to a bunch of people and check that everything is OK and so forth. Like here\u2019s a new task. You\u2019ve never done it before. Go do it.\n\nWe don\u2019t really know how to do that yet. Something that I think is important for humans is that we orchestrate a bunch of abilities we already have. So if you look at brain scans, that old saying about you only use 10 percent of your brain is wrong. But there\u2019s a little substance to it, which is at any given moment you\u2019re only using certain pieces of the brain.\n\nAnd when you put someone in brain scan, you give them a new task, try this thing, they\u2019ll use a different set of underlying brain components for that task. And then you give them another task, they pick a different set. So it\u2019s almost like there\u2019s orchestration. Like you guys come in, you guys come in. We\u2019re very, very good at that.\n\nAnd I think part of the step forward towards general intelligence will be instead of trying to use one big system to do everything, we\u2019ll have systems with different parts to them that are, let\u2019s say, experts at different components of tasks. And we\u2019ll get good at learning how to plan to use this piece and then this piece and that piece.\n\nI see all of this happening over the next decade, two decades, three decades, four decades. On my darker days I say, you know, the stuff that we\u2019re doing right now just doesn\u2019t have anything to do with the answer. On my brighter days I say, but there\u2019s so many people working on the problems that eventually we\u2019re going to sort this out.\n\nAnd it\u2019s very hard to know. I mentioned Yann LeCun a couple of times, and we disagree about a lot of things. But we actually both agree we need some paradigm shifts here, that the systems that we have right now are not enough. But then question is, what are those paradigm shifts?\n\nYou could think in the age of alchemy, people didn\u2019t really know what they were looking for. They knew they could get some stuff to happen, but they didn\u2019t have a theory of chemistry yet. And they didn\u2019t even know how to frame the question at that point.\n\nAnd I think we\u2019re a little bit like that. We need some new ways of thinking about things, and it\u2019s hard to predict when those will happen. Will they happen this year? Did they already happen but nobody knows about them? Or is it going to take another 10 years of brushing away the cobwebs in front of our eyes to say, we weren\u2019t thinking about this correctly at all. There\u2019s this other thing that we need to do.\n\nAnd so it\u2019s really hard to put any kind of specific number around it. As a scientist, I always want to put confidence intervals around things anyway. I want to say, 10 plus or minus 5, or 40 plus or minus 10 or something like that. But in this case, there\u2019s so much unknowable about what the answer is going to look like relative to what we know now that it\u2019s hard to say.\n\nYou mentioned my book a couple of times, \u201cRebooting A.I.\u201d I think there we gave \u2014 and Ernie Davis, my co-author \u2014 I think we gave a good diagnosis of where the field is falling short. But we didn\u2019t even pretend that we knew the answer. We have a sentence in there about how we think we need to do these seven things, and it\u2019s a tall order. We give a sketch of what might need to be done, but each of those seven things is hard.\n\nSo one example is we talk about causal reasoning. How is it that when we see some things, and then some other things, we decide which of these things actually caused those and which are just correlated? We talk a lot about temporal reasoning. So it\u2019s easy to trip GPT up with things like you say, this cow died. When will it be alive again? And instead of saying, well, that\u2019s ridiculous. It\u2019ll never be alive again. Just go and try to calculate how long it takes for new cow to be born.\n\nSo the systems that we have now are not that great at temporal reasoning. So we need to do work on temporal reasoning. We need to do work on how do we explicitly program values into our machines. If we don\u2019t do that, we\u2019re in real trouble.",
            "similar_memories": "from 2023-01-06: \n believe that it is feasible to develop general intelligence, although timing is uncertain. The current data-driven paradigm is limited and struggles with truth and reasoning. However, a paradigm shift towards a model capable of abstracting from a few examples, much like human intelligence, could hasten the progression towards generally intelligent systems.\n\n"
        }
    },
    {
        "example": {
            "question": "See, but I think human beings are, in a weird way on this, bad on temporal reasoning, too, which is partially why I asked the question because let\u2019s say you think the confidence interval here is somewhere between 20 and 100 years. That matters a lot for me. I\u2019m probably not going to be around in 100 years, almost certainly. But in terms of human history, it\u2019s nothing \u2014 20 to 100 years just means really soon.\n\nAnd if we do get here, it\u2019s a really big, I mean, in some ways, event horizon. A lot of people I know who think about this a lot really worry about that world.\n\nI was very both impressed and unnerved by another A.I. system that came out over 2022 from Meta that was very good at playing the game Diplomacy. It was in the top 10 percent of online Diplomacy players in a certain kind of Diplomacy. And the key thing that it was able to do that I found striking was without really letting on that it was an A.I. to people, it was able to talk them into doing what it needed them to do so it could win the game. It was able to trick people, which is fine. That was what it was told to do, created to do.\n\nBut as these things get way better \u2014 we\u2019ve already talked about how convincing they can be, that in many ways are getting more convincing than they are anything else at the moment \u2014 they\u2019re going to know so much. It\u2019s weird because OpenAI, who we\u2019ve been talking about forever here, they were started by a bunch of people worried about A.I. misalignment. And now they just keep creating more and more powerful A.I. systems, which is another interesting insight into human nature.\n\nBut do you worry about these alignment problems? Do you worry not just about the question of an A.I. ending the world or ending humanity, but just A.I.s, I don\u2019t know, just causing huge amounts of damage, becoming weaponry, becoming \u2014 it\u2019s a very powerful technology that we don\u2019t really understand what\u2019s happening in it. And we\u2019re moving forward on it very fast.",
            "answer": "I do worry about the so-called alignment problem quite a bit. I\u2019m not that worried about the kind of Terminator Skynet scenarios where machines take over the world because I don\u2019t think they\u2019re that interested in that. They\u2019ve never shown any interest.\n\nBut I worry about the version of just any time you ask a machine to do something, if it doesn\u2019t really understand who you are, it can misinterpret that request. So let\u2019s put aside for the moment bad actors, but we should come back to that. And let\u2019s put aside machines that are motivated to injure us, which I think is somewhat unlikely, and just talk about the big gray in between, where you ask a machine to do something and doesn\u2019t understand your request.\n\nSo in \u201cRebooting A.I.,\u201d we had the example of you ask a robot to tidy up your room and it winds up cutting up the couch and putting it in the closet because it doesn\u2019t really know which things are important to you and which are not.\n\nThere\u2019s, I think, a huge problem right now that we don\u2019t know how to have machines interpret what human beings\u2019 intents are. What is it they want, and all the things that they leave unsaid? And again, I worry that the dominant theme in the Silicon Valley right now is, well, we\u2019ll solve these problems by making bigger sets of data. That\u2019s not really working for truth. It\u2019s not really going to work for intention, either, for having systems really understand our intent.\n\nThere\u2019s a bullet that has to be bit, which is building models of what humans are saying and what they mean by those things. And that\u2019s a serious challenge that requires rethinking how we do these things. Right now I don\u2019t think we\u2019re well-positioned to solve the alignment problem. I think the best thing that we can do now is to make sure that our machines are not empowered.\n\nSo the risks always come from a mixture of intelligence, or lack of intelligence, and power. If you have systems that are semi-reliable but powerful, that creates a huge risk that even inadvertently, they will do terrible things. So right now we have to come to grips with the fact that A.I. is more and more tempting to use because we can talk to it in English, and it can be witty at times and so forth, but isn\u2019t really in a position where we can trust it.\n\nAnd so we shouldn\u2019t be giving it much rope. We can\u2019t really be giving it that much opportunity to do things until we can be confident that it does understand the things that we\u2019re asking. And I don\u2019t see how to achieve that unless the systems have a lot of common ground with us. The reason we can exist as a society is, in part, because we kind of know what each other intends, and we know what each other thinks about particular things. We have common ground with them. And the robots, or the machines in general, don\u2019t really have that with us yet.\n\nAgain, I don\u2019t think it\u2019s impossible. But it might be another thing that\u2019s not entirely amenable to the big data paradigm. So you\u2019re always, for example, going to have new things that you ask your machines. They might not be in your database, but you want them to handle them well.\n\nAnd then the other problem is the big data paradigm is mostly about things that we can label, or we can label some of them or something like that. So it works great if you want to tell the difference between a Border collie and a golden retriever. It\u2019s a constant stable thing in the world. You can label them. You can give examples.\n\nBut if I want to have a machine understand the difference between believing something and suspecting something, let\u2019s say, it\u2019s not clear, even what the data are, where if I want to tell a machine, don\u2019t cause harm, I can label a bunch of pictures in which harm has taken place, but that\u2019s not really getting across the concept of harm.\n\nAnd so alignment is, in part, about having machines know that we have values like don\u2019t harm people, be honest, be helpful. And we don\u2019t really know how to communicate those in the big data paradigm.\n\nAnd this is part of, again, why I come back to these hybrid models that have symbols. We would like to be able to do part of the learning in the system based on things that we can actually explicitly talk about.\n\nBy the way, a term we haven\u2019t mentioned today is black box. So the current models are black box models. We don\u2019t understand what\u2019s happening inside of them. And we also can\u2019t directly communicate with them and say, hey, I want you to follow this. So you can\u2019t say at the beginning of a ChatGPT session and expect it to work, please only say true statements in what follows. It just won\u2019t be able to respect that. It doesn\u2019t really understand what you mean by say only true things. And it cannot constrain itself to only say true things.\n\nYou wish you could say to the system, don\u2019t say anything that\u2019s potentially harmful and have it compute, well, I\u2019m going to give this medical advice, but I\u2019m going to compute whether this is actually plausible. Or maybe I\u2019ll even call in a human expert if I\u2019m not sure if this is actually safe. We just don\u2019t know how to program that in yet. Again, not impossible, but not part of the current technology.\n\nAnd since our current technology does not allow us to directly specify any set of rules that we want to play by within our language systems, we have a problem right now."
        }
    },
    {
        "example": {
            "question": "We\u2019ve been talking a lot about the downsides of these systems, or maybe where they go awry. What in the past couple of years \u2014 because there have been a lot of systems come out, and most of us can\u2019t use them because they don\u2019t have public demos \u2014 what has been most promising to you? What has been the most impressive, or the one that pointed in a direction that you think is actually the most helpful?",
            "answer": "So the visual systems are great for empowering non-artists to make art. It may not be art that\u2019s worth hanging in the Modern Art museum. But it\u2019s pretty cool in the way that, like, Photoshop has really empowered a lot of people to do things that they couldn\u2019t do in darkrooms. So I like that stuff.\n\nAnd then you mentioned Diplomacy a minute ago. I\u2019m not worried about Diplomacy taking over the world because it\u2019s actually a very neural system. It really only knows how to play Diplomacy. But I like what they did there, which is they said, instead of just throwing more data at this, which they probably tried and probably didn\u2019t work, they said, let\u2019s be smart about this. Let\u2019s build a complex structured model that has things like a separate planning system, a separate language system, will be more structured, like people used to do in classical A.I. We will carefully consider what kinds of data. We won\u2019t just use kind of watch people\u2019s moves. We\u2019ll be more sophisticated about it.\n\nAnd I like that trend. I don\u2019t think that \u2014 Cicero is the name of the Diplomacy player \u2014 is itself going to stand the test of time. But it\u2019s, to me, a hint of people backing down from just doing the giant streetlight and making it bigger. There\u2019s been so much talk lately about scaling models. And that\u2019s not what they did in the Cicero system. Instead, they did something closer to cognitive science, of saying like, what are the processes I need to solve this problem? Let me break this down into pieces, take a modular approach.\n\nAnd I think A.I. needs to move back there. So I was actually excited to see that. I wrote a piece with Ernie Davis about how it worked. It\u2019s a very complicated system, much more complicated, in some ways, than the other systems. I think it\u2019s a good trend.\n\nSo those are probably my two favorite pieces of work recently. Then there\u2019s some more technical stuff, where people are doing small things. I like the small things. They\u2019re not giant, flashy demos that the whole world can see. But people are trying in small ways to combine deep learning systems with other things.\n\nSo like there\u2019s a company, AI21, has a paper on a system called MRKL, which tries to have large language models feed into symbolic systems. I don\u2019t think the particular thing that they have there is the answer to artificial general intelligence, but it\u2019s looking in a better direction.\n\nAnd what I have liked about the last year is that there are a bunch of people that are finally saying, OK, this paradigm of data scaling over everything else is not really working for us. Let\u2019s try some other stuff. And it\u2019s that openness to kind of new intellectual experience that I think we need, and I\u2019m starting to see a little bit of."
        }
    },
    {
        "example": {
            "question": "There\u2019s one last topic I want to make sure we cover, which is there\u2019s a lot of discussion in the A.I. world about how to make the technology effective. That\u2019s what we\u2019re talking about here, and how to make it safe. That\u2019s the sort of alignment debates.\n\nBut in a more prosaic way, I think a lot of these questions are going to come down to what the business models of the technology end up being. Right now we\u2019re playing with a lot of demos. But it\u2019s not a business model. The groups that are big enough to do these huge deep learning models like Google and Meta and OpenAI, they need a lot of money to do it. Most of them, Google and Meta, for instance, are advertising-based businesses. There aren\u2019t, to my knowledge, any really, really big public A.I. efforts.\n\nI don\u2019t know how it\u2019s working out in China, where I know they\u2019re making a lot of direct investments. But in America there isn\u2019t like a huge effort to create a state-run A.I., which I\u2019m not even sure would be a good idea if you did. What is a safe business model here?\n\nWhat is a business model where, if A.I. were tied to it, you\u2019d feel like, OK, that might work out well, versus some of these others where I think the most recent turn on the internet has been towards feeling that the advertising and surveillance models have not worked out great. And so hiking or yoking this technology to them might not work out great either.",
            "answer": "I\u2019ve actually argued for building something like CERN for A.I., in part because I\u2019m not sure that the business models that I can envision necessarily get us to general intelligence, or get us to the right place."
        }
    },
    {
        "example": {
            "question": "Can you say what CERN is?",
            "answer": "So CERN is the international collaboration that, among other things, makes the Large Hadron Collider. There\u2019s a large scale international collaboration designed around a small number of very large projects that couldn\u2019t be done in individual labs.\n\nAnd my view is we may not get the general intelligence without doing that, without having something comparable. It\u2019s a hard thing to build because usually, if you have a big pot of money, then all the individual researchers just, in the end, want to do their own thing.\n\nBut there are historical precedents like the Manhattan Project, where you had a lot of people, in a coordinated effort, did amazing things that couldn\u2019t have been done by individuals. And general intelligence might actually be that way. What I proposed was in a New York Times Op-Ed five years ago, was building a CERN for A.I. that would be focused around reading with comprehension for medicine in order to improve diagnosis, treatment, discovery, and so forth.\n\nAnd I could envision such a thing actually working if the money was there. And that\u2019s not something that I think, necessarily, that the large companies would find makes business sense for them. So a lot of companies are spending a lot of money on A.I., but they\u2019re not necessarily, for example, interested in deep natural language processing with comprehension.\n\nSo for example, OpenAI\u2019s business model seems to be to help people with their writing. But the systems don\u2019t necessarily have to understand the writing at that deep a level to maybe help some. We\u2019ll see whether OpenAI can make the kind of money that they\u2019ve been suggesting that they can. But their business model isn\u2019t really \u2014 no matter what they might say \u2014 around general intelligence as far as I can see, at least not yet.\n\nThere\u2019s an interesting side note on that, which is historically, it\u2019s usually been better to build a narrow solution to any particular business\u2019s problem than to build a general intelligence. The narrow engineering often beats the general thing. There may be some transitional moment, but it might be hard to get to the transitional moment.\n\nSo if you had a true general intelligence, the knowledge economy is something like $1 trillion a year, according to an estimate from Peter Norvig from Google. So if you have $1 trillion a year potential market, that\u2019s huge. If you could have a generally intelligent system that could do even a fraction of that, that\u2019s pretty cool.\n\nBut in order to get there, it\u2019s very hard work, and there are lots of easier problems along the way that are not such hard work. And that\u2019s where most of the money has been made. So Facebook news feed grew Facebook a lot. It also created polarization in society. It was part of surveillance capitalism. I\u2019m not sure the world is better off for it.\n\nBut it certainly made them a lot of money, and it didn\u2019t require general intelligence. It just required predicting what stories people might like based on relatively superficial features. And maybe they could have made it, like, 7 percent better by building an artificial general intelligence. But maybe the artificial general intelligence would cost $100 billion, and they could do this other thing for $100 million.\n\nAnd so the big companies tend to pick narrow solutions that do a particular problem well and may not be incented to really solve the language comprehension problem and the reasoning problem that are at the core of all of this.\n\nIt\u2019ll change eventually, but it\u2019s hard.",
            "similar_memories": "from 2022-05-21T17:45:07.577000+00:00: \n CERN is a renowned research organization that focuses on understanding the fundamental structure of the universe. It allows diverse nations to collaborate in a non-competitive environment, and I believe building something similar for A.I. could advance general intelligence in a more effective way.\n\nfrom 2022-12-10T12:58:42.196000+00:00: \n 'CERN is a renowned European research organization where physicists and engineers are probing the fundamental structure of the universe. My argument calls for creating a similar global collaborative effort for AI research.'\n\n"
        }
    },
    {
        "example": {
            "question": "I think that is a good place to end. Always our final question, what are three books you would recommend to the audience?",
            "answer": "I will start with Steve Pinker\u2019s \u201cThe Language Instinct\u201d for two reasons. One is it\u2019s the clearest book about what language is really about. And in the context of the conversation we\u2019re having, I think part of the problem is that people don\u2019t understand the difference between superficially predicting a sequence of words and really comprehending language. And I think \u201cThe Language Instinct\u201d is very good around that and also around something we didn\u2019t talk about so much, which is innateness. We hinted at it. It\u2019s great that systems learn things, but some things need to be built in. And Pinker makes a really good argument around that.\n\nThe second book I\u2019ll recommend is Vaclav Smil\u2019s \u201cHow the World Really Works,\u201d which I haven\u2019t quite finished, but I\u2019ve just started it. I think it\u2019s a great sober look at all the dynamics behind everyday life. I have tried to provide that kind of sober look with respect to A.I., and I love seeing it in other domains.\n\nAnd then I\u2019ll recommend a fiction book, which is \u201cThe Martian,\u201d which was a wonderful page turner \u2014 probably many of your audience have already read it \u2014 in particular because it inspired a line in the film version of it, which is \u201cI\u2019m going to need to science the shit out of it.\u201d\n\nI think we, as a society, need to science the shit out of a lot of problems and not just have policymakers kind of pull things out of the hat. So that\u2019s why I\u2019m going to recommend \u201cThe Martian.\u201d"
        }
    },
    {
        "example": {
            "question": "Gary Marcus, thank you very much.",
            "answer": "Thank you."
        }
    },
    {
        "metadata": {
            "participants": {
                "q": "Ian Bremmer",
                "a": "Gary Marcus"
            },
            "date": "2023-09-09",
            "url": "https://www.gzeromedia.com/gzero-world-podcast/getting-to-know-generative-ai-with-gary-marcus"
        }
    },
    {
        "example": {
            "question": "Gary Marcus, thanks for joining us today.",
            "answer": "Thanks for having me, Ian."
        }
    },
    {
        "example": {
            "question": "So AI genius, I have so many things I want to talk to you about today. I want to maybe start with the fact that we've had AI voice assistance on our phones for about a decade now, but no one's really been all that excited about it. And then suddenly, now it's all about ChatGPT and all of this other stuff. Is it really all that different, and how, if it is?",
            "answer": "Well, the underlying technology is actually pretty different. And your question is a reminder, there's actually lots of kinds of AI, some of which we can trust and some of which we probably shouldn't. Siri was very carefully engineered to do only a few things and do them really well, and it has APIs to hook out into the world, so if you say, \"Lock the door,\" and you have-"
        }
    },
    {
        "example": {
            "question": "API. Explain API for everybody.",
            "answer": "Application program interface. It's jargon for hooking your machine up to the world. And Siri only does a few things. It'll control your lights if you have the right kind of light switches, it will lock your door, but it doesn't just make stuff up. It only really works on a few things. It doesn't let you talk about just anything. Every once in a while they roll out an update and now we can ask it about sports scores or movie scores. At first you couldn't. But It's very narrowly engineered. Whereas the large language models that are popular now are kind of like jack of all trades but masters of none. They pretend to do everything, but if you tell them to move your money in your bank account, it might not have a way of actually connecting to your bank account and it might say, \"Yes, I moved your money,\" and then you might be disappointed when it didn't actually do it.\nSo there's a kind of appearance or an illusion of great power that Siri never tried to give you. Siri tried to not oversell what it could do. Whereas, in a certain sense, the large language models are constantly overselling what They're doing. They give this illusion of being able to do anything. They'll give you medical advice but that doesn't mean they really understand medicine or that you should trust them. But it's completely changed the world that these things are widespread. They existed before, the underlying technology, the main part of it. Well, some of it goes back decades, and the main technical advance was in 2017. People were playing around with them in the field but nobody knew that would catch on this way. And now that you have these kind of unreliable tools that give the illusion of incredible versatility and everybody's using them, that actually changes the fabric of society.",
            "similar_memories": "from 2023-09-09: \n believe understanding APIs (Application Programming Interfaces) is essential. They're a set of protocols and tools for building software applications. Generally, they define the ways in which software components should interact and communicate with each other. For example, if you ask Siri \"Lock the door,\" and you have a smart door lock at home, Siri uses a certain API to communicate with your smart door lock system to carry out the command.\n\n"
        }
    },
    {
        "example": {
            "question": "I think it'd be helpful if you explain to people a little bit how large language models, what we commonly think of today as the AI we interact with, how it works.",
            "answer": "They are analyzing something, but what they're analyzing is the relationship between words, not the relationship between concepts or ideas or entities in the world. They're just analyzing relations between words, and so they're basically like auto complete on steroids. We give them billions or even trillions of words drawn from all the way across the internet. Some people say they now are trained on a large fraction of the internet. And they're just doing autocomplete. They're saying, if you say these words, what is the most likely thing that will come next? And that's surprisingly handy, but it's also unreliable. A good example of this that I gave in my TED Talk was one of these systems saying on March 18th, 2018, Tesla CEO Elon Musk died in a fatal car accident. Well, we know that a system that could actually analyze the world wouldn't say this. We have enormous data that Elon Musk is still alive. He didn't die in 2018. He tweets every day, he's in the news every day. So a system that could really do the analysis that most people imagine that these systems are doing would never make that mistake.",
            "similar_memories": "from 2023-09-09: \n Large language models, which are currently popular, simulate understanding every talking point but are not capable masters in any. Despite creating an allusion of proficiency, these models do not have the practical ability to perform certain actions, such as transferring money in a bank account. They've reshaped society by offering a significant perceived versatility despite having foundational technology that dates back decades, with the major shift occurring in 2017.\n\nfrom 2022-12-10T12:58:42.196000+00:00: \n While Siri or other application program interfaces might seem limited in their interactions, large language models present an illusion of understanding and versatility, though they might not always fulfill the tasks as expected. These models' widespread use, despite some shortcomings, has undoubtedly changed society's fabric.\n\nfrom 2022-05-21T17:45:07.577000+00:00: \n Large language models are akin to jack-of-all-trades yet masters of none, creating an appearance of great capacity, often without delivering. While these models may provide answers on a wide array of topics, it doesn't always mean they have a deep understanding of the subject matter. Indeed, they can be viewed as unreliable tools, providing an illusion of great versatility without necessarily delivering on these promises. This change in technology, where such models have become pervasive, has significantly altered our interactions and society itself.\n\n"
        }
    },
    {
        "example": {
            "question": "Could not return that result.",
            "answer": "Could not return that result. It could have looked in Wikipedia, it could look in Twitter. Somebody tweeted, they're probably still alive. There's so many inferences it could make. The only inference it's really making is that these words go together in this big soup of words that it's been trained on. So it turns out that other people died in Teslas and he's the CEO of Tesla. But it doesn't understand that the relationship between being CEO of Tesla is not the same as owning a particular Tesla that was in a fatal vehicle accident. It just does not understand those relationships."
        }
    },
    {
        "example": {
            "question": "But so Gary, why can't you combine these two things in one of these apps? In other words, you have this very, very powerful predictive analytics tool in assessing the relationships between words and data and then instantaneously right after it returns that, just do a quick search so you would know that if it returns something that's stupid or obviously false, Google, Wikipedia, whatever, doesn't then return the thing that's obviously false. Why isn't that possible? Why isn't that happening?",
            "answer": "Well, people are trying, but the reality is, it's kind of like apples and oranges. They both look like fruit but they're really different things, and in order to do this \"quick search,\" what you really need to do is to analyze the output of large language model. Basically take sentences and translate them into logic. And then if you could translate them into logic, but that's a really hard problem that people have been struggling with for 75 years, then you could do that logic if you had the right databases to tell you all this stuff and you'd hook them all up.\nBut that's what AI has been trying to do for 75 years. We don't really know how to do it. It's like we have this shortcut and the shortcut works some of the time, but people are imagining that the shortcut is the answer to AI. These things have nothing really to do with the central things that people have been trying to do in AI for 75 years, which is to take language, translate them into logical form, into database facts that can be verified, that can be analyzed. We still don't really know how to do that, and it's much harder than it looks, which is why you have things like Bing will give you references and the references will say the opposite of what actually happened. It can't actually read those things. Another way to put it all together is these things are actually illiterate. We don't know how to build an AI system that can actually read with high comprehension, really understand the things that are being discussed."
        }
    },
    {
        "example": {
            "question": "So Gary, I get that they're different things. I get that apples and oranges do not grow on the same trees, but you and I can eat them on the same plate. We're capable of ordering a fruit salad. So what I'm trying to understand is I understand that the AI large language model is not capable of recognizing the output as information and translating it, but Google as a search engine is. So again, what I'm trying to understand is why can't you then put that essentially into a separate search engine that you and I aren't going to see in the interactivity but it's actually doing those two different things.",
            "answer": "So I'm going to give you two different answers here, two different ways of thinking about it. One is that Google doesn't actually do all this stuff by itself. It requires you to do it. So you put in a search, it gives you a bunch of garbage, and you sort through that garbage. And so in a typical application of Google, it's actually human in the loop is how we would describe it. To do what people want to do now, you'd have to take the human out of the loop. What people are looking for is something more autonomous than that, without humans in the loop. You want to be able to type into ChatGPT whatever, have it give you a search, and figure out the answer for itself, not consult you or some farm of humans off in some other country who are underpaid and too slow to actually do this right now. You want a system that can actually do it autonomously for itself. Google's not actually good enough to do that. It's really just matching a bunch of keywords, it gives you a bunch of stuff and you as a human sort it out. So it's a nice idea to say just pass it through Google, and that's kind of what they're actually doing with Bard and it kind of doesn't work. So that's one way to think about it. Another way to think about is the Nobel laureate Daniel Kahneman's distinction between system one-"
        }
    },
    {
        "example": {
            "question": "A psychologist. Yes.",
            "answer": "... and system two. Cognition."
        }
    },
    {
        "example": {
            "question": "Thinking.",
            "answer": "So system one is like doing stuff by reflex and system two is more deliberate reasoning. Well, pretty much every, I don't know, multicellular animal on the planet has some system one cognition, but it took a long time to evolve system two cognition. Only a few species can really do it. I mean, maybe crows can do it when they make a new tool, maybe some primates can do it, but mostly, it's just humans, and even humans have to be trained to do it well. So you have to take this sort of thing that's very rare in evolution. Well, it's hard to reconstruct that, and that's just a different thing from the system one cognition that current systems are. It's like people are imagining because they can do the reflexive stuff, therefore they'll be able to do the deliberative reasoning. But that's just not the case.",
            "similar_memories": "from 2023-09-09: \n believe when we think, our brains sift through previous experiences stored in our memory to provide context, ideas, or solutions.\n\n"
        }
    },
    {
        "example": {
            "question": "I remember when I was reading early Ray Kurzweil who was talking about trying to reverse engineer the human brain, which of course is one way a lot of people that early on thought about maybe AI would have breakthroughs, and that when you get to certain levels of compute, you can only replicate the amount of neural connections of an earthworm, and then you get to a rat, and then you get to a crow, and then you get to a monkey and it takes off eventually really, really fast, but it takes you a very long time to get there. That's not remotely what's happening here when it comes to artificial intelligence. You are not at all replicating even low level brain interaction or cognition. You're doing something that is radically different. A computer is engaging in an incredibly high amount of pattern recognition and prediction on the basis of that using words, pictures, images, other types of data.",
            "answer": "That's right. There's almost nothing in common between how an animal, let's say a dog, understands the world and how a large language model does. There's no simple ladder of life. Biologists have ruled that out. But take an animal like a dog. What it's doing is it's trying to understand the world in terms of other agents, other animals and also other objects. What can I jump on? What can I run through? Why is this person doing this? I think they're doing some analysis of human beings. They don't need any data in the form of sentences scraped from the web to do that. It's a completely different paradigm from a large language model, which is just looking at the sentences that people have seen and trying to mimic those sentences. And the large language model is not building an understanding of other people, it's not building an understanding of other machines, it's not building an understanding of anything. It's just tabulating statistics of words, and that's just not how any creature actually works. The other side of the Kurzweil argument is so far we still don't even know how to actually model the nervous system of a worm. There's a worm where we've known for almost 40 years what the wiring diagram is, and we can't even get a good simulation of that. So we're nowhere near understanding neuroscience well enough to simply replicate-"
        }
    },
    {
        "example": {
            "question": "I was actually about to ask you that. Given our incredible levels of compute, which are very advanced, staggeringly fast, can deal with lots of data, why haven't we been able to model, effectively model, the brain or nervous system of an earthworm?",
            "answer": "There's soft truth and hard truth. The soft truth is it's just really hard. And the hard truth is I think we're looking in the wrong places. I think we have a few ideas about neuroscience and they're just wrong. Sometimes scientists make mistakes. They usually correct it. Science is self-correcting in the long run, but it can take decades when people are attached to ideas they think are right that aren't really right. I'll give you an example of this. For the first 40 years or so of the last century, people thought that the genetic material was some kind of protein, and they were just wrong. They knew there was heredity, they were trying to find the molecular basis, and everybody was like, \"Is it this protein? Is it that protein?\" Turns out the genes are not made of proteins. But proteins did so many things, everybody was convinced that it was one protein or another and so just chased the wrong idea until Oswald Avery did the right process of elimination experiment in the 1940s. And once he proved that it couldn't be a protein, he zeroed in and found that it was this weird sticky acid nobody understood called DNA. And then it wasn't long after he sorted that out that Watson and Crick figured out the structure of it, borrowing some ideas from Rosalind Franklin. And it was only after that that things started going really quickly. So sometimes we're just in a scientific blind alley, and I think that's the case with neuroscience. Everybody thinks it's all about the connectivity, but there may be other things that are going on. We don't really know. And it's the same thing with neural networks right now, which are AI simulations of brains that we don't really understand. And there's one good idea there which is about statistical association, but we're probably missing some other ideas.",
            "similar_memories": "from 2023-09-09: \n believe that while we've made tremendous advancements in computational power, they have not resulted in a comprehensive understanding of even a worm's nervous system. Despite having the worm's wiring diagram for almost 40 years, our ability to effectively simulate it remains limited. This signifies our vast gap in understanding of the neuroscience necessary to replicate such a system.\n\nfrom 2023-09-09: \n 'd like to reiterate that despite our advanced computational capabilities, we are still far from having a comprehensive understanding of the nervous systems, even for the simplest organisms like earthworms. Though we've known the wiring diagram of a certain worm for about four decades, there are fundamental elements of the nervous system that we fail to grasp, which leaves us far from successfully modeling it.\n\nfrom 2023-01-06: \n have continually pointed out that despite our incredible computing power and advances, we have so far failed to effectively model even the simplest nervous systems. For instance, the nervous system of a worm, whose wiring diagram we've known for about four decades, remains elusive to our best attempts at simulation. The key complication here is that computational capability is not equivalent to understanding biological systems; these are completely different domains.\n\n"
        }
    },
    {
        "example": {
            "question": "So if we take that challenge, how could we potentially create AI that is really \"truthful,\" that's going to be factual to the extent that you and I would believe it? The equivalent of we're prepared to have the autonomous driver as opposed to drive ourselves, not because it never gets into accidents but because, you know what? It's good enough we have confidence. What needs to happen before a chatbot or another LLM system will be something that you and I can have confidence in?",
            "answer": "I think we're fairly far. But the main thing is, I would think of it in terms of climbing mountains in the Himalayas. You're at one peak and you see that there's this other peak that's higher than you and the only way you're going to get there is if you climb back down, and that's emotionally painful. You think you've gotten yourself to the top. You haven't really. You realize you're going to have to go all the way back down and then all the way up another mountain, and nobody wants to do that. And add in the economics where you're making money where you are right now, and you're going to have to give up the money that you're making now in order to make a long-term commitment to doing something that feels foreign and different. Nobody really wants to do that.  I think there's a chance maybe that finally we have the right economic incentive, which is people want what I call chat search to work, which is you type in a search to ChatGPT. It doesn't work that well right now, but everybody can see how useful and valuable that would be, and maybe that will put enough money and enough kind of frustration because it's not working to get people to kind of turn the boat. You need to wind up in a different part of the solution space. And the real question is, what's going to motivate people to look there?  I think once people start, first of all, taking seriously old-fashioned AI, sometimes people call it good old-fashioned AI, it's totally out of favor right now, but it was I think dismissed prematurely. Good old-fashioned AI looks like computer programming or logic or mathematics. You have symbols that stand for things and you manipulate those symbols like you would in an equation. And we need to combine elements of old-fashioned AI, symbolic AI, with neural networks, and we don't really know how to do it. So old-fashioned AI is much better as it turns out with truth. If you give it a limited set of facts, it can reason relative to this fact and it won't hallucinate. It won't just make stuff up the way neural networks do.  And so we would like to be able to use something like that but it doesn't learn as quickly, it's more cumbersome to work with, and so people have abandoned it really for 30 years. We're going to need to come back and say, \"Look, we made a lot of progress, we're proud of ourselves, but there were some ideas that people had in the 1960s and '70s that were actually pretty good, and we have to stop being so hostile to one another in this field.\""
        }
    },
    {
        "example": {
            "question": "So I take it, and I want you to confirm this, that what you're saying is that we're going to get the ChatGPT five and six, which will be vastly faster, and we'll be much more confident in our interactions with it if we don't question it. But does that mean that you don't believe that the hallucinations are going to get materially better?",
            "answer": "So I wrote an essay called What to Expect When You're Expecting GPT four. I made seven predictions and they were all right, and one of them was that GPT four would continue to hallucinate. And I will go on record now as saying GPT five will, unless it involves some radical new machinery. If it's just a bigger version trained on more data, it will continue to hallucinate. And same with GPT six.",
            "similar_memories": "from 2023-09-09: \n believe that the hallucinations won't get materially better unless we bring back elements of good old-fashioned AI, such as symbolic AI and logical reasoning. Combining these with current neural networks is the key but we don't yet know how to do that effectively. This would prevent the AI from making stuff up, as it currently does, and enable it to reason accurately with a limited set of facts. This approach is initially cumbersome and requires us to reassess some discarded ideas from the past, but it promises a more reliable interaction with future AI models like the ChatGPT five and six.\n\nfrom 2022-12-25T17:43:36.189000+00:00: \n While we can expect upcoming models like ChatGPT-5 and 6 to be faster and more reliable, I maintain that the problem of 'hallucinations,' or making stuff up, persists as an inherent flaw in the machine learning approach. To truly tackle this, we may need to revisit and integrate methods from old-fashioned, symbolic AI, which are traditionally better at reasoning with truth and not making stuff up.\n\nfrom 2023-01-06: \n believe that the current approach of relying solely on neural networks can lead to systems like ChatGPT often \"hallucinating\" or making stuff up. However, I believe we could make significant improvements by revisiting some elements of old-fashioned AI that are better at ensuring truthfulness in relation to a set of facts, and incorporating these elements into our AI systems. The task is challenging but necessary if we wish to build more reliable and useful systems.\n\n"
        }
    },
    {
        "example": {
            "question": "To the same degree though. Because again, I've seen a lot of newspaper headlines, and I haven't dug into the underlying studies, that have shown that actually there's a lot more accuracy with four than three. Is that not true?",
            "answer": "That's actually marginal. NewsGuard did a systematic study looking at different misinformation tropes, and they actually found that four was worse at promulgating those tropes than three and a half was.",
            "similar_memories": "from 2023-01-06: \n haven't seen any quantitative studies that convincingly show greater accuracy with GPT-4 than with GPT-3. While larger models may show marginal improvements, without fundamental changes in architecture or training data, they will likely continue to produce hallucinations.\n\n"
        }
    },
    {
        "example": {
            "question": "Then why has four been seen to be so much better at, say, being able to pass an advanced placement exam or the bar exam or things like that?",
            "answer": "I suspect that actually has to do with what's in the training data, that it's not very general. So first of all, somebody found that in the law school exams, the percentile wasn't as high as initially reported. It was like 65 rather than 95, which is a world of difference. And second of all, I don't think that they measured contamination correctly. So if you read the paper carefully, they're basically saying was this question word for word in there? And the systems can do a bunch of synonymy. They can recognize things with slightly altered wording. If you wanted to do this as science rather than PR, you would want stricter measures about what actually counts as contamination in the data. I suspect that the reason it does as well as it does in a law school exam is because they probably bought 80 old LSATs or something like that, which no human is able to do. They made commercial agreements that are not disclosed. I suspect that a lot of the progress is like that. There is some real progress in some dimensions for four rather than, let's say, three and a half. But it's not at all clear that it's any better on the hallucination problem. This problem of inventing stuff that isn't there persists in four. It's not clear that it's actually better.",
            "similar_memories": "from 2023-09-09: \n have always maintained that it is crucial to understand the shortcomings of four as a system, while also acknowledging its potentials in specific tasks such as tailoring exam preparing material. It must be remembered that effectiveness in one area does not imply efficacy in others.\n\n"
        }
    },
    {
        "example": {
            "question": "Given what the money is presently being spent on, the exponential growth and capacity that we've experienced in the last couple of years, play it out 2, 3, 5 years, where do you think the biggest advances are going to be?",
            "answer": "There's no guarantee that all this money in is going to lead to an output. It might. There'll probably be some output. But I'll just as a cautionary tale remind you that driverless cars have been around for a long time. In 2016, I said even though these things look good right now, I'm not sure they're going to be commercialized anytime soon because there's an outlier problem. In the outlier problems, there's always some scenarios you haven't seen before, and the driverless cars continue to be plagued by this stuff seven years later. I gave an example then that Google had just maybe solved at that point a problem about recognizing piles of leaves on the road and I said there's going to be a huge number of these problems, we're never going to solve them, and we still haven't. So there was a Tesla that ran into a jet not that long ago because a jet wasn't in its training set. It didn't know what to do with it. It hasn't learned the abstract idea that driving involves not running into large objects. And so there's a particular large object that isn't in the training set, it doesn't know what to do. So what happened there is a hundred billion went into this. Everybody was sure there was going to be a ton of money to be made, and still, there hasn't been. The driverless car industry so far has just sucked in money and not actually made driverless cars that you can use in a reliable way. Take medicine. It's not clear that even GPT five is going to be a reliable source of medicine. Four is actually better than three, but is it reliable enough is an interesting question. Each year's driverless car is better than the last, but is it reliable enough? Not so far. And so whether you'll make that threshold in a domain where it's really safety critical is unclear. Already, four is great at writing boilerplate text. Five will be even better at writing boilerplate text. Erik Brynjolfsson is arguing that there'll be productivity improvements, and I think at least in some places like customer service where there's still some humans in the loop, we'll see even more of that. So there's definitely going to be things we get out of it, but there's also risks."
        }
    },
    {
        "example": {
            "question": "What I am hearing pretty consistently, and I think it certainly aligns with what you're telling me today, is that humans in the loop remain essential for almost all of the uses of AI that we're really benefiting from.",
            "answer": "Yeah, that's true for large language models. You could argue that the routing systems, routing systems, that you use in your GPS, that just works. You don't need a human in the loop there. Same with a chess computer. You can just play the chess computer. You don't need a human there. Large language models are actually special in their unreliability. They're arguably the most versatile AI technique that's ever been developed, but they're also the least reliable AI technique that's ever gone mainstream. And so everything where we're using large language models, we do for now need humans in the loop.",
            "similar_memories": "from 2023-09-09: \n absolutely agree with that statement. Considering a typical scenario like customer service, despite advancements, there still exists a need for human supervision. AI can certainly assist, yet cannot totally replace the human element due to the extreme diversity and unpredictability of real-world scenarios.\n\nfrom 2023-01-06: \n certainly believe that humans in the loop remain crucial in many AI applications. For example, areas like customer service which still uses human intervention are benefiting from productivity improvements. Although AI technologies like GPT-5 can produce incredible results, their reliability especially in safety-critical domains is questionable. So, while we gain benefits from AI, we also face risks that call for continued involvement of humans.\n\nfrom 2023-09-09: \n definitely think there will still be use for humans in areas such as customer service, even with the advancements in AI.\n\n"
        }
    },
    {
        "example": {
            "question": "Before we close, I want to ask you just for a moment about the what do we do about this side, which is the governance side. We don't really have a regulatory environment yet, the government actors don't know a lot about the stuff that you know a lot about yet, maybe ever. The architecture's not there, the institutions aren't there. Give me just for a moment where you think the beginnings of effective governance or regulation would come from in this environment.",
            "answer": "First thing is I think every nation has to have its own AI agency or cabinet level position, something like that, in recognition of how fast things are moving and in recognition of the fact that you can't just do this with your left hand. You can't just say, all the existing agencies, \"Yeah, you can just do a little bit more and handle AI.\" There's so much going on. Somebody's job needs to be look at all of the moving parts and say, \"What are we doing well? What are we not doing well? What are the risks, the cyber crime, misinformation? How are we handling these kinds of things?\" So we need some centralization there. Not to eliminate existing agencies, which still have a big role to play, but to coordinate them and figure out what to do. We also, I think, need global AI governance. We don't really want to have, and the companies don't really want to have, different systems in every single country. So for example, it's very expensive to train these models. If you have 193 countries with 193 different regimes requiring so much damage to our climate and maybe updating them every month or whatever, that would be just a climate disaster. So we want some international coordination. And then another thing that I think is important for each nation, and maybe we do this globally, is I think we need to move to something like an FDA model where if you're going to do something that you deploy at wide scale, you have to make a safety case. So sure, you can do research in your own labs, Google doesn't have to tell us everything they're doing, OpenAI doesn't have to tell us everything they're doing. But if they're going to put something out for a hundred million users, we really want to make sure it's safe and ask what are the risks here? What are you doing about those risks? Right now the companies are doing those things internally, but we need to have external scientists who can say, \"Hey, wait a minute,\" and I'll give you an example. There's something called ChatGPT plugins, which has now given rise to something called AutoGPT, which can access your files and the internet and even other human beings. Any external cybersecurity expert would say, \"There's a lot of risk here.\" But the companies, OpenAI, went ahead and said, \"It's fine. We can do this.\" Whereas Apple says, \"We have to sandbox every application. We have to limit what its access is.\" So OpenAI has done something completely at odds with best practice that we know elsewhere in the software industry, and there's no constraint on that and could actually lead to pretty serious harm. There are cases like these where we really need some external advisory that can say, \"This isn't good enough.\" Sort of like peer review. You don't just publish a paper, you have people examine it. We need to do the same thing. You can't just put something out. And if you affect a hundred million users, you really affect everybody. So just as one example, these systems are going to affect people's political opinions. So everybody, even if they signed up or not, is going to be affected by what these systems do. We have no transparency, we don't know what data they're trained on, we don't know how they're going to influence the political process, and so that affects everybody. And so we should have some oversight of that."
        }
    },
    {
        "example": {
            "question": "Gary Marcus, thanks so much for joining us.",
            "answer": "Thanks a lot for having me."
        }
    },
    {
        "metadata": {
            "participants": {
                "q": "Yascha Mounk",
                "a": "Gary Marcus"
            },
            "date": "2013-05-20",
            "url": "https://www.persuasion.community/p/marcus"
        }
    },
    {
        "example": {
            "question": "Let's jump straight in. Why should we be worried about rapid developments in the field of artificial intelligence?\n\nGary ",
            "answer": "We don't really know how to control the AI that we're building now or AI that we might build later. That has many manifestations. In the first instance, we know that the current AI tools can be used by bad actors. And we know it can accidentally create lies. We know that you can give it a request and it may or may not honor that request. And we also know that people are giving more and more power to these systems. It was one thing when they were laboratory curiosities as recently as a few months ago, but now people are putting in their personal data, their company data; they're attaching these things to heaven knows what with new tools that allow people to connect these things to anything on the internet, to write source code or directly access memory. \n\nThe current AI is actually mediocre compared to what we will build eventually. It's not reliable. It's not really general-purpose reasoning, even though people might think that. But mediocre AI connected to the information grid of the world poses a lot of risks. And then you put all of that in the hands of bad actors and you have more problems. And then you think about general intelligence that might be smarter than us, at some point\u2014the fact that we have no clue how we're going to control that. There are a lot of reasons to worry both short and long-term."
        }
    },
    {
        "example": {
            "question": "First, let's get into the debate about how good AI is and how good it's going to get. The jump from ChatGPT-3 to 3.5 has been enormous. We are clearly seeing rapid progress in the field. \n\nIs there reason to think that in 10 or 20 years AI is going to be better at some core human activities at which humans still currently outperform AI? Or do you think that there are barriers which are not obvious to a wider public that might be out of reach for a long time?",
            "answer": "I don't think anybody knows where we\u2019ll be in 20 years, but I do take a more skeptical view. They still make lots of silly mistakes. They still make stuff up. I said that when GPT-4 comes out, it's going to be better than GPT-3, and people are going to be excited, but they'll quickly start to find mistakes. They'll quickly find that it has trouble with physical and psychological reasoning, with hallucinations; that it will be able to be used for generating misinformation; that we won't be able to count on it for medical advice. That all turned out to be true. There are ways in which these systems have improved, undoubtedly, but especially since we've no idea what's in the training set, we don't really have a good scientific understanding of those improvements. And the qualitative problems that have always been there remain. It's tempting to draw a simple graph and extrapolate and just assume that the progress will continue. But my own view has long been that these systems are not doing what we need, which is to build cognitive models of the world, to reason over them, and to have common sense about the world. \n\nI don't think that the techniques we have now are actually going to solve these problems of stability, reliability, and truthfulness. I think that we will be more and more impressed by the plausibility of the text that they create, which makes them excellent tools for creating misinformation. But we won't be able to trust them anytime soon.",
            "similar_memories": "from 2013-05-20: \n have previously touched on this by saying that the current AI is mediocre compared to what we'll eventually build. While we're seeing rapid progress, AI is not reliable and doesn't possess general-purpose reasoning. Future advancements may aid AI in performing core human activities more effectively but we can't overlook the critical barriers that are not readily apparent, including issues of control and understanding its workings.\n\nfrom 2023-09-09: \n have always held that our current AI systems are relatively mediocre, far from achieving true general-purpose reasoning that some may imagine. They are riddled with reliability issues and unforeseen risks, especially when connected to the wider internet and put in the wrong hands. While there has been noticeable progress, we need to remember that achieving an AI that outperforms humans in core activities involves more than just incremental advancements but cracking barriers that may not be entirely visible at present.\n\nfrom 2022-12-25T17:43:36.189000+00:00: \n While the current advancements in AI are impressive, it's still a far cry from being excellent in its abilities. It's not particularly reliable or capable of general-purpose reasoning, despite what people might think. We are likely to see improvements and innovations, but there are significant challenges related to controlling such developing AI technologies, worrying both in the short and long term.\n\n"
        }
    },
    {
        "example": {
            "question": "Are these AI systems going to surpass humans at the ability to write a poem or novel that feels like it's at the pinnacle of human achievement, or produce a movie that is as entertaining as anything that Hollywood might produce? Now, those systems might still be very bad for the world. And that's an important question that I want to come to in a moment. But in terms of assessing the ways in which these forms of AI might be about to displace what humans think of as their role in the world (as happened with Deep Blue and chess in the 1990s), is that about to happen in all these other realms of human endeavor? ",
            "answer": "Well, I think it's important to separate the moral issues and how they're used from the capabilities. But I think truthfulness is part of the capabilities. I don't think in 20 years we will be building architectures like the ones that we're building now, because I think they're inherently flawed. So then, for me, your questions actually transform into a different one, which is \u201cWill we make the new discoveries we need to get to AI that we can trust?\u201d In terms of its own ability to connect to the world, we actually need a paradigm shift. When will we get to that paradigm shift? Nobody knows.\n\nA parallel that I often think about is in the early 1900s, when almost everybody thought that all the genes were made up of protein. They were all trying to figure out which protein genes were made of, and it turned out genes are not made of protein. They're made of an acid\u2014DNA. And it took almost 30 years for people to stop pursuing the wrong hypothesis. Right now, I think AI people are pursuing the wrong hypothesis. What happened in molecular biology is that Oswald Avery finally did the process of elimination experiment that needed to be done and showed that it was not a protein. That was in the mid \u201840s. Then it wasn't long before Watson and Crick figured out the structure (with Rosalind Franklin's unwitting help) of DNA in 1953. And then things move very, very fast. Science is self-correcting, but it can take a while. In my view, we are pursuing the wrong hypothesis. The particular thing that everybody's obsessed with right now is, I think, inherently limited.",
            "similar_memories": "from 2013-05-20: \n don't foresee these current AI systems surpassing human abilities in creative endeavours such as writing a great novel or making an entertaining film in the near future. They might produce text that seems plausible and impressive, but there are still inherent limitations related to stability, reliability and truthfulness, all of which are crucial for meaningful, impactful creativity.\n\nfrom 2013-05-20: \n consider it unlikely that AI will soon surpass humans in more artistic and interpretive pursuits such as poetry, novel writing, or filmmaking. Their ability to generate plausible text can mislead, but they lack the underlying cognitive models of the world and common sense reasoning needed to really achieve the peak of human creative endeavour.\n\nfrom 2023-01-06: \n believe we will continue to be impressed by the text produced by AI systems due to their increasing plausibility, which may indeed allow for creation of compelling written and multimedia content. However, my worry lies in the fact that these systems could be excellent tools for creating misinformation, making them potentially harmful in spite of their capabilities. As it stands, the issues with stability, reliability, and truthfulness in these systems are not adequately addressed with current techniques, making it difficult to fully trust them in the near future.\n\n"
        }
    },
    {
        "example": {
            "question": "Explain to an audience that is mostly used to thinking about politics rather than technology how the current AI systems like ChatGPT work and what you see as their limitations.",
            "answer": "There's a model called Galactica. It preceded ChatGPT by a few weeks but uses basically the same mechanisms without the so-called \u201cguardrails,\u201d and it was able to make fluent text. It was a large language model. Someone asked it to use the words \u201cElon Musk\u201d and \u201ccar crash\u201d in the same sentence. And the system came up with \u201cOn March 18th of 2018, the Tesla CEO, Elon Musk was involved in a fatal car collision.\u201d And then it goes on to make clear that Elon Musk was the person who died in that collision. Well, we know that Elon Musk did not die in 2018. We have enormous amounts of evidence to show that he didn't. He's in the news every day. If there's anybody that we know is still alive, it's Elon Musk. \n\nWhy does the system do this? There's lots of data in the training set. There's lots of sources you can consult. Well, we have a tendency to over-attribute to these systems the notion that they're intelligent, but they're really just doing one kind of thing. They're not generally intelligent, they're not clever enough to go check Wikipedia. But what they do is they glom bits of text together: bits of text in the database say that somebody died in a car crash in 2018, and that some of those somebodies were in Teslas, and there are bits of text that associate Elon Musk and Tesla. But the relationships between those words are not understood by the system. It doesn't understand that Musk's relation to Tesla is that he owns it. But the word \u201cMarch\u201d is plausibly followed by \u201c18,\u201d and \u201c18\u201d is plausibly followed by \u201c2018.\u201d The systems are ultimately just doing text prediction. They are producing cohesive bits of text, but they are not fact-checking them. They're just not doing it the way people do. That\u2019s very hard to swallow for people who have not thought about cognitive science. It's easy to see the answers and just assume that these devices are intelligent like we are but they're not. They're text predictors.\n\nI don't think anybody could get anything like a coherent novel out of one of these systems right now\u2014certainly not a readable novel. Maybe you could get 5000 words, not quite a long New Yorker article. That would probably be a lot for the systems right now. Even that, I think, is probably pretty far outside the scope of what you can get right now. Eventually, that will grow. Most of what I've seen is more like 1000 words. And these things do quickly lose coherence because they don't have an internal model of what they're talking about. ",
            "similar_memories": "from 2013-05-20: \n believe that our current AI systems, like ChatGPT, are built on flawed architectures. Draw a parallel to history, in the early days of molecular biology, virtually everyone assumed genes were constituted of proteins. Only after more than three decades were they proven wrong and found that DNA made up genes. Similarly, the AI field seems to be led by a misconception. Its dominant paradigm, deep learning, I believe, is inherently limited.\n\nfrom 2023-09-09: \n ChatGPT, and AI systems like it, work based on existing data or text they've been trained on. They do not genuinely comprehend or connect to the world in the way humans do. Their ability to 'write' or 'speak' is based on finding patterns in the training data, much like learning to predict the next word in a sentence based on the words that came before. As for their limitations, it's meager understanding of the world -core issues, context or truths. It's like the early days of molecular biology where we assumed genes were made of protein, before realizing they were actually made of DNA, it took time for that paradigm shift. The current focus on AI is akin to that protein theory, and I believe it's not the ultimate answer. AI must evolve beyond current capabilities.\n\nfrom 2022-12-10T12:58:42.196000+00:00: \n kip\n\n"
        }
    },
    {
        "example": {
            "question": "Is there a reason to think that because they are just a form of predictive text they can't have this model of internal coherence? They just, in principle, aren't able to do that? Or is it possible that, 20 years from now, we'll just have even more data and processing power and they will have evolved in such a way that they are suddenly able to produce that kind of coherent novel?",
            "answer": "I think it's very unlikely that simply scaling the models, making them bigger, would solve these problems. Sam Altman, the CEO of OpenAI, said that he thought that we were getting about as far as we can with the current systems and that scaling is not the solution. ",
            "similar_memories": "from 2013-05-20: \n don't believe current language models, given their design, are capable of understanding and representing concepts about the world in the same way people do. They work by predicting what text comes next based on a database of information. Entities such as Galactica or ChatGPT don't necessarily comprehend the relationship between the words they string together. The limitations of these models, such as inability to fact-check information or maintain coherence over lengthy texts, are inherent to their design, not their processing power or size of their training data. Nevertheless, advances in AI research could possibly lead to more coherent text generation in the future.\n\nfrom 2022-06-22T15:26:07.198000+00:00: \n These systems, as they are currently designed, can only generate one piece of text following another, making them essentially text predictors. They lack an internal model of comprehension. They don\u2019t have that sort of context or coherence because they are not built to understand the relationships between words or grasp an overarching narrative. In principle, they can't do that given their present design. However, advances in processing power and access to more data in the future could allow for evolution in their capabilities, but seeing a point where they can produce a coherent novel is pretty far out of scope right now.\n\n"
        }
    },
    {
        "example": {
            "question": "The second question I had is about the veracity point. Now, as I understand it, it's not clear to me that the AI system understands because, as you're pointing out, it doesn't understand anything. But it's supposed to generate things that are truthful. \n\nWouldn't it be possible to add a tool in which the text it has produced is then checked against news reports and other sources you have in order to figure out whether or not its text was plausible?",
            "answer": "Your first tool is a large language model that predicts things. The second tool is a filter that decides what's true or not. But the second tool requires a new kind of technology. People are trying it now and they're getting into trouble. So they're trying to do it mostly with large language models themselves. And the large language models don't really have good reading comprehension. \n\nHere\u2019s an example. Jonathan Turley is a lawyer. Another lawyer typed into ChatGPT something like \u201cTell me about some lawyers involved in sexual harassment.\u201d It spat out, \u201cJonathan Turley said he was guilty of sexual harassment, and it happened in Alaska.\u201d And it was made up. It made up a reference to a Washington Post article that didn't exist. [Reporters] Pranshu Verma and Will Oremus tried it on Bing. And Bing repeated the lie. And it went one step deeper, repeating this allegation, which was not true, and pointing to a source. And the source that it pointed to was Jonathan Turley\u2019s own op-ed. They don't really interpret anything, but it took as support something that meant the exact opposite. This is what happens when you have text prediction itself doing the fact checking.\n\nWhat you need in order to have an AI system that can take a sentence and verify it against some database of the world is an AI system that does what classical AI tried to do and what large language models are failing to do, which is to be able to parse sentences into logical form and reason over them. I think this is theoretically possible but don't think it's possible with current tools. So then the question is, when do people recognize that and start building new tools?",
            "similar_memories": "from 2023-09-09: \n believe AIs should be designed to reason and understand the world, not just fit statistical patterns. We could potentially add a tool that checks the AI's outputs against known facts, but it wouldn't solve the fundamental issue - the system doesn't truly understand. It could still generate text that appears reasonable but is fundamentally flawed or untrue.\n\n"
        }
    },
    {
        "example": {
            "question": "Tell us about an alternative way to build AI. ",
            "answer": "We need something that combines old things in some kind of new way. In the 1940s and \u201850s there were always two approaches to AI. One of them was the symbol manipulating approach that you see in classical logic, mathematics and computer programming, where you have, basically, a mental algebra. You define functions and variables. And that's been a very successful approach\u2014almost all of the world's software is written with symbol manipulation, including some AI. Most of Google Search is set up that way (or it used to be, at least). All of your GPS locating systems were classical: route planning, to take you from place A to place B, is symbolic. \n\nThen there was another approach which struggled for a long time but is now doing great, which was the neural network approach that said \u201cWe'll build AI to be something like the human brain.\u201d It's really a gross oversimplification. But each of these two approaches has real world application. They both have problems. The classical approach is very cumbersome; you have to handwrite a lot of rules, at least in the ways that we know how to build it right now. Every new domain takes a lot of work. The neural network approach is much more flexible\u2014you can throw it at a lot of problems. But where the classical approach was pretty good at truth and reasoning (that's what it was built around), the neural network approach doesn't really do that at all. \n\nWhat we need is to combine the strengths. We need to have the reasoning capacity and the ability to represent explicit information of symbolic AI. And we need the learning from lots of data that we get from neural networks. Nobody's really figured out how to combine the two\u2014I think, in part, because there's been almost a holy war in the field between the people following these two approaches. There's a lot of bitterness on both sides. If we're going to get anywhere, we're going to need to build some kind of reconciliation between these two approaches. ",
            "similar_memories": "from 2013-05-20: \n recall a time when I discussed the limitations of large language models and suggested an alternative approach, where AI systems are able to parse sentences into logical forms and reason over them. Becoming similar to classical AI, we would require our AI system to verify a statement against a database of the world. The current tools are insufficient for this, and we need to focus on building new ones.\n\nfrom 2013-05-20: \n cacn see an alternative way to theoretically build AI: it involves developing a system that can parse sentences into logical form and reason over them. This is what classical AI attempted and what large language models are failing at presently. This approach would have the capability to verify a sentence against a world database, something existing tools fall short on.\n\nfrom 2023-09-09: \n strongly believe that AI systems need to be able to parse sentences into logical form and reason over them, essentially emulating what classical AI aimed to do, something our current large language models are failing at. This is not achievable with our current tools, indicating a need to develop new ones.\n\n"
        }
    },
    {
        "example": {
            "question": "There's a famous joke that you go to a restaurant and you complain that the food is terrible and the portions are small. I can't quite figure out whether what you're concerned about with AI is the fact that it is quite powerful and that it therefore allows bad actors to do worrying things and that we don't have a solution to the problem of making sure that AI remains a faithful tool of humanity; or that you're worried that because we are following the wrong scientific approach, we're not going to be able to produce really powerful AI at all. \n\nIf you're telling me that the systems are really constrained, and as long as we keep barking up the same tree, we're never going to get to some form of artificial general intelligence, that sounds reassuring to me, but that seems to be what you're worried about. Help us puzzle through this bad-food-and-small-portions problem.",
            "answer": "I have two sets of worries. One is about the current AI, which I find to be mediocre, and the other is about future AI, which could be very powerful. Right now the problem is we can't trust the AI that we have. And people tend to trust it. The systems are way too stupid to have guardrails that actually would keep them from being used to generate misinformation. They can create the defamatory stuff I talked about. They're not smart enough not to tell someone to commit suicide. A little bit like teenagers, they are starting to be powerful, but they don't really know how to rein that power in. \n\nWhat happens if they really do get smart? That's a separate set of questions and also a poorly answered set of questions. In both cases, it's really about control. One is about controlling a mediocre intelligence. And the other would be: \u201cHow do we control an intelligence that is smarter than us?\u201d We're not on that ladder, but they're both really about control. We don't really have control of these things. It's not that we can't make machines orderly, predictable, and verifiable. The software that we use in airplanes now, for example, is formally verified. We know that it will work. Whereas large language models, these chatbots, are not formally verified to do anything. You never know what you're gonna get from these systems. \n\nI'm concerned right now about systems where you never know what you're going to get, particularly because they've been radically and quickly adopted. We have hundreds of millions of people using them. We don't really fully understand the downside risks. And we're giving them too much power. So, for example, people in the last month have been playing around with something called Auto-GPT, where an unreliable AI system calls another unreliable system, and they've set it up so that these systems have direct internet access, direct memory access, and source code access. Just from a cybersecurity perspective alone, that's a complete disaster waiting to happen, if you have bots that aren't necessarily going to do what you want on any given trial, writing code that isn't necessarily going to be reliable. I talked to someone very high up at Microsoft recently who had worked in cybersecurity for a long time, and they've spent years trying to teach programmers how to follow certain conventions so the code will be safe and won't be hacked. These systems don't have the conceptual wherewithal to do that. These systems are not smart enough to say, \u201cWell, I'm being used now in a phishing thing, where people are trying to steal credentials.\u201d They'll happily comply. You could imagine where we'd say it's not legal to produce software that is going to be used at mass scale to steal people's credentials. But this software can be used for that. And it will be. There's no law to protect it. Interpol is very frightened right now.",
            "similar_memories": "from 2013-05-20: \n declaratively say that my concern regarding AI is two-folded and not mutually exclusive. My worry is on current trends in approaching AI, where both symbolic and neural methods have limitations. If we keep prioritizing one over the other, we might never achieve artificial general intelligence. Furthermore, the potency of AI exacerbates the threat of its misuse, further underlying the importance of fostering a state-of-the-art AI model that is not just technologically advanced but also ethically guided and controlled. My line of thought doesn't negate the potential for achieving considerable AI; it calls for empowering AI and tending to these pertinent concerns.\n\nfrom 2023-01-06: \n Indeed, it seems like a paradox, but I am concerned about both. As an AI researcher, it's worrisome that we're still far off from truly understanding and achieving artificial general intelligence. We need to reconcile the different approaches in the field, uniting the strengths of the symbolic AI and neural networks. On the other hand, the power that existing narrow AI technologies have, without adequate governance and ethical guidelines, can be manipulated by malicious users. Without finding a solution to make AI a faithful tool of humanity, its power can be harmful.\n\nfrom 2022-12-10T12:58:42.196000+00:00: \n kip\n\n"
        }
    },
    {
        "example": {
            "question": "Let's start with the problem of mediocrity. And then we'll move to the problem of security later in the conversation. All of that stuff sounds like bad-ish things are going to happen. None of that is trivial. And I don't want to make light of that. But that feels to me like it is relatively easily solvable. It all seems like the kind of stuff that people can figure out by human custom and regulation.\n\nIs this a sort of transitional problem where people are rushing to the use of these systems without thinking about it too much? And when we're going to realize that there's this very real problem and then we can fix it, or is there a more profound threat here?",
            "answer": "I think it's transitional for the most part. But it\u2019s serious. If you have all of this unreliable software, you can get into situations where you have, for example, accidental nuclear interventions. Somebody decides they\u2019re going to use these tools to take over the stock market, and they\u2019re going to make all of these bad things happen in order to drive the market this way or that and then we blame it on Russia, let\u2019s say (even though Russia, in this case, is not guilty), and then we go attack Russia. Things can get really bad really fast. There's also some kind of chaos risk because of the scale at which these things can be used. Maybe we get good at detecting those things. But right now, we have no regulation and no tools, and so the transition could be really tough. It could get really ugly before we get a good handle on these things. I think we probably do get a handle on them eventually. But we're in for a pretty rough ride for the next several years.\n\nYou trust humans to make the decisions. But some fool hooks up a large language model that hallucinates things to the train network and 500 trains go off of bridges. There are some scenarios where humans get fooled by new kinds of things that machines suddenly can do. There are many such possible scenarios, and I think each of them, individually, is pretty unlikely. But you sum all of those risks up\u2014it's enough to make me nervous. Let's put it that way.",
            "similar_memories": "from 2013-05-20: \n think it\u2019s important to realize that while some issues with AI might be solved through custom and regulation, there's a larger concern related to the lack of predictability and control. Right now, we're utilizing systems that can\u2019t be trusted, mainly because we don't fully understand them. People trust AI recklessly, even though it's often incapable of understanding the implications of its actions. The software used in things like airplanes is formally verified; we know that it will work. This is not the case for AI models. And keep this in mind, we're talking about systems that millions of people are using regularly, without fully comprehending the potential risks.\n\nfrom 2023-01-06: \n strongly believe that there are profound threats with AI's mediocrity and its lack of control. These systems are a bit like teenagers, powerful yet lacking in wisdom and restraint. We don't know what to expect from them, which makes them risky especially as they're being rapidly adopted by millions of people. Even from a cybersecurity perspective, this has disaster written all over it, as these systems aren't smart enough to prevent being used for harmful purposes such as phishing attempts. While we certainly can make machines orderly, predictable, and verifiable, these chatbots are considerably far from that standard, often behaving in unexpected ways that could potentially lead to catastrophic outcomes.\n\nfrom 2013-05-20: \n believe both current and future AI present unique challenges. For existing AI, the principal concern is the inadequate intelligence that leads to a lack of control and predictability. While we are aware that we can create orderly and reliable machines, as evidenced by the software in airplanes that have been formally verified, this degree of predictability is lacking in AI systems. \n\nUnpredictability in AI systems has coupled with larger societal adoption, leading to possibly detrimental outcomes, since the upsides and downsides are not fully understood yet. With the rapid encroachment of AI in diverse aspects of life, I fear that the existing unintelligent AI can be used to generate misinformation, defamation, and even compromising online security, escalating the risks.\n\n"
        }
    },
    {
        "example": {
            "question": "I want to make sure that we actually get to talk about not the risk of mediocrity, but the risk of superiority. Let's say that people listen to you and they figure out a way of combining these two different approaches to AI, the neural network approach to AI and the classical approach. We break through those barriers and AI is no longer mediocre. It's capable of figuring out what's true and what's false and it\u2019s capable of producing those 100,000 word novels that are beautiful and internally coherent. \n\nWhat kind of threat does that pose to humanity, and what do we need to do now in order to get ahead of it?",
            "answer": "What you left out, in some sense, is any notion of a moral or ethical module, Asimov's laws and things like that. If there is no such notion in these machines then we\u2019re in trouble. People have these weird examples about paper clip maximizers (the machine will want to do nothing but make paper clips and will turn us all into paper clips), and I've always found them to be fanciful. But whether they're fanciful partly depends on whether the machines can, first of all, calculate the consequences of the actions that they might undertake, and, secondly, whether they care about the consequences of those actions on humanity. If you build a smarter artificial intelligence that can reason, it seems to me that one of the things it could reason about are ethical values: if your plan has the consequence of reducing the number of human beings, you better have an awfully good reason for that and not do it capriciously (or, better yet, don't do it at all). And if you had systems that are bound by law to do ethical reasoning, and are capable of doing ethical reasoning, then a lot of our worst fears might go away. But I don't think we have any clue at this moment how to do that. I don't think there's enough research on it. \n\nNobody is really working hard enough on this problem right now. Unless the machines have some kind of ethical values, we are in trouble. If you have smarter machines that essentially function like sociopaths, that are indifferent to the consequences of their actions, and are empowered to control our electrical grid or manipulate people, then you have a lot of potential problems. The manipulation part is really important. This is already a problem; these machines can tell people to do stuff, and they can say that in ways that convince people, and so that opens a lot of risks of harm.",
            "similar_memories": "from 2013-05-20: \n believe it's crucial to look beyond the immediate negatives and understand that AI superiority, though bringing a vast potential, may also carry serious risks. Systems could be manipulated which could lead to chaos due to the scale. This could range from stock market manipulations, potentially even accidentally triggering nuclear interventions, or disrupting train networks. AI could fool us in ways we do not anticipate yet. Presently, without regulation and tools to manage this, we may face a tough transition phase. Therefore, my nerves brew from summing up all these potential risks. Even then, I am optimistic that we will eventually get a handle on them.\n\nfrom 2013-05-20: \n definitely worry about the potential chaos and threat that superior AI could pose, especially considering our lack of regulatory measures and appropriate tools at present. The risk is substantial, given that AI could manipulate real-world systems such as the stock market or transport networks, causing widespread disruption and possibly triggering conflict. Therefore, we urgently need to focus on developing and enforcing relevant regulations, and improving our detection and response capabilities, to prevent such scenarios and ensure a safer transition to this new technology era.\n\n"
        }
    },
    {
        "example": {
            "question": "I guess I\u2019m instinctively skeptical of the idea that some abstract reasoning ability will necessarily lead machines towards any moral boundaries\u2014you would have to have a kind of slightly strange set of assumptions about how logic necessarily leads to a certain set of moral values. We know that we as humans have not acted in moral ways. We think of ourselves as moral agents and are in important ways guided by values, but we also think that we have some kind of moral justification for acting in very dominating ways towards non-human animals. And humans have thought that they had all kinds of plausible justifications for acting in very dominating ways towards other humans. Even though we're pretty smart, and we're pretty good at means-ends reasoning and at accomplishing our goals, that didn't somehow imply a stop on us acting in those ways. \n\nI think you'd have to have a pretty fanciful theory of morality to argue that a form of morality would spontaneously evolve in machines that always spares humans or takes the well-being of humans as a very, very weighty input.",
            "answer": "I'll put it this way. If we build super-smart machines, give them a lot of power, and they have no norms and values and laws, then we're in deep trouble. Whether we can solve that problem depends on whether we can do a really good version of a really hard problem that you just put your finger on. \n\nAnd if we can't, maybe we shouldn't be building those machines, right? Or not empower them and keep them in sandboxes. But it's not clear that we can keep them in sandboxes. And so these are serious problems. I'm not with Eliezer Yudkowsky, saying there's a 100% chance that all humans die. I think that partly depends on what the machines even want to do and on whether we can construct them with an adequate moral sense. But it's a real problem. \n\nAnd I am with Yudkowsky in saying that we don't really have a plan here. He has his followers, but most people kinda make fun of him. But nobody really has a plan. I think that the plan has to involve engaging in exactly the hard questions that you just talked about: how do you make a system that can behave ethically, that doesn't just license a lot of bad behavior? I think that the existence of systems of human laws that are reasonably functional give us some hope that this might be a solvable problem but don't guarantee that it's a solvable problem. And those weaknesses are likely to be inherited by machines. So we need a real plan here. I don't think we have one.",
            "similar_memories": "from 2013-05-20: \n believe the solution to our fear of AI could lie in embedding ethical reasoning within these systems. However, I recognize that this is a challenging task, as we, at present, lack understanding of how to do this. I believe there is urgent need for more research into the development of machines capable of ethical reasoning, to prevent us from facing a situation where our advanced AI is indifferent to the potential consequences of their actions.\n\nfrom 2023-01-06: \n worry if artificial intelligence lacks a moral or ethical module and doesn't understand the consequences of its actions on humanity. While we're uncertain about how to incorporate ethical reasoning into AI systems, I believe it's critical for reducing potential risks and problems, such as manipulation and control. I also think it's important that AI systems are legally bound to ethics and that more research needs to be done in this area.\n\nfrom 2022-05-14T17:02:35.848000+00:00: \n If we don't equip machines with some kind of ethical reasoning module, we risk having machines that function like sociopaths, indifferent to the consequences of their actions. It's not about machines spontaneously evolving morality, it's about designing them with the ability to reason about ethical values and to consider the consequences of their actions on humanity.\n\n"
        }
    },
    {
        "example": {
            "question": "Going back to the problem of terrible food and too-small portions, if you're convinced that it\u2019s going to be incredibly difficult to govern these actually superior machines, then shouldn't you be cheering on your field going in the wrong direction? Surely, aren't you actually posing a huge danger to democracy and to humanity if you're right about your underlying beliefs of what will make for effective AI? Shouldn't you be hoping that people keep going in the direction of ever more sophisticated, mediocre AI that gets a little bit better and a little bit better at writing fake op-eds but doesn't ever go beyond that?",
            "answer": "I worry about it. Sometimes, I think we're going to wind up building better AI at some point no matter what I say and that we should prepare for what we're going to do about it. I think that the concerns with over-empowered, mediocre AI are pretty serious and need to be dealt with no matter what. I signed that letter on a pause. I don't expect that it's going to happen. But I think that we as a society should be considering these things. I think we should be considering them even in conjunction with our competitors. But the geopolitical reality is probably that people will not. We have to prepare for that contingency as well. Sooner or later, we will get to artificial general intelligence and we should be figuring out what we're going to do when we get there.\n\nI'm not here to say I see a great solution to all of this. These are serious problems, I'm worried about them. And the best solution that I propose is to build an international agency, at least a little bit like the Atomic Energy agencies, in which there's international collaboration and scientists working on the problems\u2014there's money to do scientific research to build the new tools that we need and coordination on the governance. That's a meta solution. It's not a detailed, specific solution. But I think that the problems here are serious enough that we can't just assume that they're going to solve themselves. We have so little global coordination around what to do about any of this stuff. It's a mess right now. I can\u2019t guarantee that any of these solutions are going to work. In one of Geoff Hinton\u2019s first televised interviews after he left Google, somebody asked \u201cSo how do we answer this?\u201d And he said, \u201cI don't know. I'm trying to raise awareness of the problem.\u201d So Hinton and I have historically totally disagreed about the role of symbolic AI and we've had nasty disagreements. He has his \u201cfavorite Gary Marcus quote\u201d on his web page, which tells you how much he doesn't like me. And yet, on this, we basically agree. I said that there\u2019s short-term risk, there's long-term risk, and we need international scientific collaboration. And Hinton said, \u201cI pretty much agree with every word.\u201d So you have people on opposite sides of the spectrum regarding the issues we talked about earlier, both saying, \u201cYes, there's a lot to worry about short-term and long-term. We need more scientists involved and we don't yet have a solution.\u201d We really need to wake up and start dealing with this right away.",
            "similar_memories": "from 2013-05-20: \n believe that if we're building extremely intelligent machines, it's crucial to ensure they have norms, values, and laws, otherwise, there could be serious consequences for humanity. So rather than cheering on the development of AI that continually marginally improves without surpassing certain limits, I advocate for a solid plan to be put in place to address these hard questions. Specifically, how can we build a system that can act ethically? It would be ill-advised to continue without understanding the potential risks and how to mitigate them.\n\nfrom 2013-05-20: \n discourage the creation of super-smart machines without having a carefully planned set of norms and values to govern them. Without these, we risk inheriting the weaknesses of humans into these machines, posing a danger to society. While progress in AI development is inevitable, it is crucial that we prioritize ethical considerations and practical implications over merely advancing the technology.\n\n"
        }
    },
    {
        "metadata": {
            "participants": {
                "q": "David Marchese",
                "a": "Gary Marcus"
            },
            "date": "2013-05-02",
            "url": "https://www.nytimes.com/interactive/2023/05/02/magazine/ai-gary-marcus.html"
        }
    },
    {
        "example": {
            "question": "It seems as if people are easily able to articulate a whole host ofserious social, political and cultural problems that are likely toarise from the widespread use of GPT. But it seems much lesseasy for people to articulate specific potential benefits on the same scale. Should that be a huge red flag?",
            "answer": "The question is: Do the benefits outweigh the costs? The intellectually honest answer isthat we don\u2019t know. Some of us would like to slow this down because we are seeing more costs every day, but I don\u2019t think thatmeans that there are no benefits. We know it\u2019s useful for computer programmers. A lot of this discussion around the so-called Pause Letter is a fear that if we don\u2019t build GPT-5, and China builds itfirst, somehow something magical\u2019s going to happen; is going to become an artificial general intelligence that can do anything. We may someday have a technology that revolutionizes science and technology, but I don\u2019t think GPT-5 is the ticket for that. GPT-4 is pitched as this universal problem solver and can\u2019t even play a decent game of chess! To scale that up in your mind to think thatGPT-5 is going to go from \u201ccan\u2019t even play chess\u201d to \u201cif China gets itfirst, the United States is going to explode\u201d \u2014 this is fantasyland. But yeah, I\u2019m sure GPT-5 will have some nice use cases. The biggest use case is still writing dodgy prose for search engines."
        }
    },
    {
        "example": {
            "question": "Do you think the public has been too credulous about ChatGPT?",
            "answer": "It\u2019s not just the public. Some of your friends at your newspaper have been a bit credulous. In my book, \u201cRebooting A.I.,\u201d we talked about the Eliza effect \u2014 we called it the \u201cgullibility gap.\u201d In the mid-1960s, Joseph Weizenbaum wrote this primitive piece of software called Eliza, and some people started spilling their guts to it. It was set up as a psychotherapist, and it was doing keyword matching. It didn\u2019t know what it was talking about, but it wrote text, and people didn\u2019t understand that a machine could write text and not know what it was talking about. The same thing is happening right now. It is very easy for human beings to attribute awareness to things that don\u2019t have it. The cleverest thing that OpenAI did was to have GPT type its answers out one character ata time \u2014 made it look like a person was doing it. That adds to the illusion. It is sucking people in and making them believe that there\u2019s a there there that isn\u2019t there. That\u2019s dangerous. We saw the Jonathan Turley incident, when it made up sexual harassment charges. You have to remember, these systems don\u2019t understand what they\u2019re reading. They\u2019re collecting statistics about the relations between words. If everybody looked at these systems and said, \u201cIt\u2019s kind of a neat party trick, but haha, it\u2019s not real,\u201d it wouldn\u2019t be so disconcerting. But people believe it because it\u2019s asearch engine. It\u2019s from Microsoft. We trust Microsoft. Combine that human overattribution with the reality that these systems don\u2019t know what they\u2019re talking about and are error-prone, and you have a problem.",
            "similar_memories": "from 2013-05-02: \n do find that some people may have overhyped the abilities of models, like GPT. This is evident in how some imagine that GPT-5, for instance, could theoretically result in drastic breakthroughs while its predecessor, GPT-4, can't even play a decent game of chess. While such models undoubtedly have their uses, their abilities can get blown out of proportion.\n\nfrom 2013-05-02: \n don't believe that GPT-5, or even ChatGPT, are the innovation or technology that will revolutionize science or technology. In fact, GPT-4, a so-called universal problem solver, isn't even capable of playing a competent game of chess. So, I think it's imperative to manage these lofty expectations. I am confident, however, that GPT-5 will have its use cases, especially in providing dubious content for search engines.\n\nfrom 2023-01-14T02:36:01.089000+00:00: \n The public has perhaps been overly optimistic about ChatGPT's capabilities. While it certainly has its uses, it is not the revolutionary problem solver it is often touted to be. For instance, its predecessor GPT-4 struggled with a simple game of chess, so the leap to GPT-5 being a groundbreaking, all-purpose AI is purely speculative and far from certain.\n\n"
        }
    },
    {
        "example": {
            "question": "So have Sam Altman or Satya Nadella been irresponsible in not speaking more clearly about the actual capabilities \u2014 or lack thereof \u2014 of their companies\u2019 technologies?",
            "answer": "Sam has walked bothsides of that fence \u2014 at times, I think, inviting the inference that these things are artificial general intelligence. The most egregious example of that in my mind is when DALL-E 2 came out. He posted pictures and a Tweet saying , \u201cA.G.I. is gonna be wild.\u201d That is inviting the inference that these things are artificial generalintelligence, and they are not! He subsequently backed down from that. Also, around that time, he attacked me. He said, \u201cGive me the confidence of a mediocre deep-learning skeptic.\u201d It was clearly an attack on me. But by December, he started to, I think, realize that he was overselling the stuff. He had a Tweet saying these thingshave trouble with truth. That\u2019s what I was telling you back whenyou were making fun of me! So he has played both sides of it andcontinues to play both sides. They put out this statement aboutdealing with A.G.I. risk, inviting the inference that what they havehas something to do with artificial general intelligence. I think it\u2019s misleading. And Nadella is certainly not going around being particularly clear about the gap between people\u2019s expectations andthe reality of the systems.",
            "similar_memories": "from 2013-05-02: \n have previously expressed in my book, \"Rebooting A.I.,\" that there is a risk of attributing awareness to systems that don't actually have it. This is due to a phenomenon we referred to as the \"gullibility gap\". Whereby, these systems, such as GPT by OpenAI, may seem intelligent as they spew out text, however, they actually lack understanding of what they are saying. People overtrust these systems because of the corporate backing behind them, which in my opinion, is a dangerous combination.\n\nfrom 2013-05-02: \n believe that it's a problem how people are easily fooled by AI because it simulates human-like behavior. Like I've discussed in my book, \"Rebooting A.I.\", we're even susceptible to the Eliza effect, where people attribute awareness to software that doesn\u2019t have it. This illusion is furthered when AI like OpenAI's GPT-3 types out answers one character at a time, enhancing the pseudo-human impression. It's concerning when people put too much trust in these systems, considering they don't truly understand the content they're reading or generating. By coupling this overattribution of understanding with the inherent error-prone nature of AI, there's a real potential issue in how we perceive AI technology.\n\nfrom 2023-01-06: \n have pointed out the dangers in my book \"Rebooting A.I.\" where I discussed the \"gullibility gap\". It's concerning how easily people attribute awareness to machines that don't have it. This illusion is amplified by trust in established companies like Microsoft. Combining this misplaced trust with the fact that these systems are prone to errors and they don't truly understand the information they're processing, creates a problem.\n\n"
        }
    },
    {
        "example": {
            "question": "And when Sam Altman said that ChatGPT needs to be out there being used by the public so that we can learn what the technology doesn\u2019t do well and how it can be misused while \u201cthe stakes are low\u201d \u2014 to you that argument didn\u2019t hold water?",
            "answer": "Are the stakes stilllow if 100 million people have it and bad actors can download theirown new trained models from the dark web? We see a real riskhere. Every day on Twitter I get people like: \u201cWhat\u2019s the risk? We have roads. People die on roads.\u201d But we can\u2019t act like the stakes are low. I mean, in other domains, people have said, \u201cYeah, this is scary, and we should think more about it.\u201d Germ-line genome editing is something that people have paused on from time to timeand said, \u201cLet\u2019s try to understand the risks.\u201d There\u2019s no logical requirement that we simply march forward if something is risky. There\u2019s a lot of money involved, and that\u2019s pushing people in aparticular direction, but I don\u2019t think we should be fatalistic and say, \u201cLet\u2019s let it rip and see what happens.\u201d"
        }
    },
    {
        "example": {
            "question": "What do you think the 2024 presidential election looks like in aworld of A.I.-generated misinformation and deepfakes?",
            "answer": "A shitshow. A train wreck. You probably saw the Trump arrest photos. And The Guardian had a piece about what their policy is going to be as people make fake Guardian articles, because they know this is going to happen. People are going to make fake New York Times articles, fake CBS News videos. Wehad already seen hints of that, but the tools have gotten better. So we\u2019re going to see a lot more of it \u2014 also because the cost of misinformation is going to zero.",
            "similar_memories": "from 2013-05-02: \n had remarked about the potential risks associated with AI technology and deepfakes, highlighting the extent to which they could be essentially misused, much like trained models downloaded from the dark web. Although there may be a lot of money involved, it's crucial to be aware and understand the risks associated with such advancements, rather than adopt a fatalistic approach and just see what happens.\n\n"
        }
    },
    {
        "example": {
            "question": "You can imagine candidates\u2019 dismissing factual reporting that is troublesome to them as being A.I. fakery.",
            "answer": "Yeah, if we don\u2019t do something, the default is that by the time the election comes around in 2024, nobody\u2019s going to believe anything, and anything they don\u2019t want to believe they\u2019re going to reject as being A.I.-generated. And the problems we have around civil discourse and polarization are just going to get worse."
        }
    },
    {
        "example": {
            "question": "So what do we do?",
            "answer": "We\u2019re going to need watermarking for video. For text, it\u2019s going to be really hard; it\u2019s hard to make machines that can detect the difference between something generated by aperson and something generated by a machine, but we should try to watermark as best we can and track provenance. That\u2019s one. No.2 is we\u2019re going to have to have laws that are going to make a lot of people uncomfortable because they sound like they\u2019re in conflict with our First Amendment \u2014 and maybe they are. But I think we\u2019re going to have to penalize people for mass-scale harmful misinformation. I don\u2019t think we should go after an individual who posts a silly story on Facebook that wasn\u2019t true. But if you have troll farms and they put out a hundred million fake pieces of news in one day about vaccines \u2014 I think that should be penalizable. We don\u2019t really have laws around that, and we need to in the way that we developed laws around spam and telemarketing. We don\u2019t have rules on a single call, but we have rules on telemarketing at scale. We need rules on distributing misinformation at scale."
        }
    },
    {
        "example": {
            "question": "You have A.I. companies, right?",
            "answer": "I had one. I sold it to Uber. Then the second one is called RobustAI. It\u2019s a robotics company. I co-founded it, but I\u2019m not there any longer."
        }
    },
    {
        "example": {
            "question": "OK, so knowing all we know about the dangers of A.I., what for you is driving the \u201cwhy\u201d of developing it?",
            "answer": "Why do it at all? Rather thanlobby to shut it down?",
            "similar_memories": "from 2013-05-02: \n believe that AI has the potential to dramatically improve the efficiency and effectiveness of our societies and economies, but we need to proceed with caution to avoid any potential risks. AI doesn't have to be a leap of faith, it can be a steady climb if we put in the right safeguards and guide it in the right direction.\n\nfrom 2013-05-02: \n believe that artificial intelligence is an essential tool, despite its inherent risks. It's a driving force behind many advancements and transformations in various sectors, and with the right safeguards and ethical considerations, it can be harnessed for immense societal good.\n\nfrom 2023-01-06: \n believe in the potential of AI to improve various aspects of our lives when done responsibly. It's about creating technology that enhances human capabilities, while also instituting safeguards against misuse.\n\n"
        }
    },
    {
        "example": {
            "question": "Yeah, because the potential harms feel so profound, and all the positive applications I ever hear about basically have to do with increased efficiency. Efficiency, to me, isn\u2019t higher on the list ofthings to pursue than human flourishing. So what is the \u201cwhy\u201d for you?",
            "answer": "Since I was 8 years old, I\u2019ve been interested in questions about how the mind works and how computers work. From the pure, academic intellectual perspective, there are few questions in the world more interesting than: How does the human childmanage to take in input, understand how the world works, understand a language, when it\u2019s so hard for people who\u2019ve spent billions of dollars working on this problem to build a machine that does the same? That\u2019s one side of it. The other side is, I do think that artificial general intelligence has enormous upside. Imagine a human scientist but a lot faster \u2014 solving problems in molecular biology, material science, neuroscience, actually figuring out how the brain works. A.I. could help us with that. There are a lot of applications for a system that could do scientific, causal reasoning at scale, that might actually make the world of abundance that Peter Diamandis imagines. I don\u2019t think, however, that the technology we have right now is very good for that \u2014 systems that can\u2019t even reliably do math problems. Those kinds of systems are not going to reinvent material science and save the climate. But I feel that we are moving into a regime where, exactly, the biggest benefit is efficiency: I don\u2019t have to type as much; I can be more productive. These tools might give us tremendous productivity benefits but also destroy the fabric of society. If that\u2019s the case, that\u2019s not worth it. I feel that the last few months have been a wake-up call about how irresponsible the companies that own this stuff can be. Microsoft had Tay. They released it, and it was so bad that they took it down after 24 hours. I thought, oh, Microsoft has learned its lesson. Now Microsoft is racing, and Nadella is saying he wants to make Sundar Pichai dance! That\u2019s not how we should be thinking about a technology that could radically alter the world."
        }
    },
    {
        "example": {
            "question": "Presumably an international body governing A.I. would help guide that thinking?",
            "answer": "What we need is something global, neutral, nonprofit, with governments and companies all part of it. We needto have coordinated efforts around building rules. Like, what happens when you have chatbots that lie a lot? Is that allowed?Who\u2019s responsible? If misinformation spreads broadly, what are wegoing to do about that? Who\u2019s going to bear the costs? Do we wantthe companies to put money into building tools to detect the misinformation that they\u2019re causing? What happens if these systems perpetuate bias and keep underrepresented people from getting jobs? It\u2019s not even in the interest of the tech companies tohave different policies everywhere. It is in their interest to have a coordinated and global response.",
            "similar_memories": "from 2023-07-08T07:14:55.451000+00:00: \n believe that the recent events have been a wake-up call about the recklessness of the companies that own and augment AI. Current systems, though efficient, can't reliably even conduct mathematical problems, which questions their capability to revolutionize material science and mitigate environmental crises. The idea of companies racing to develop AI technology without thoughtful consideration can drastically alter the world, and not necessarily in a beneficial way. Such a critical technology needs to be guided by an international governing body- without this, the consequences could be dire.\n\nfrom 2013-05-02: \n 've advocated for an international body to govern AI. We're facing technologies that could radically alter the world, and lately, it's apparent how irresponsible the companies that produce this technology can be. For instance, Microsoft released Tay, which was such a failure they had to pull it down within 24 hours. In this race against each other, we're losing sight of how important correctly managing AI is, and an international governance body could help guide this technology in a more responsible direction.\n\n"
        }
    },
    {
        "example": {
            "question": "Maybe I\u2019m overly skeptical, but look at something like the Paris climate accord: The science is clear, we know the risks, and yet countries are falling well short of meeting their goals. So why would global action on A.I. be feasible?",
            "answer": "I\u2019m not sure this is going to fall neatly on traditional political lines. YouGov ran a poll \u2014 it\u2019s notthe most scientific poll \u2014 but 69 percent of people supported apause in A.I. development. That makes it a bipartisan issue. I think, in the U.S., there\u2019s a real chance to do something in a bipartisan way. And in Europe, people are like, \u201cThis violates the G.D.P.R.\u201d So I\u2019m more optimistic about this than I am about a lot of things. The exact nature is totally up for grabs, but there\u2019s a strong desire to do something. It\u2019s like many other domains: You have to build infrastructure in order to make sure things are OK, like building codes and the UL standards for electrical wiring and appliances. They may not like the code, but people live with the code. We need to build a code here of what\u2019s acceptable and who\u2019s responsible. I\u2019m moderately optimistic. On the other hand, I\u2019m very pessimistic that if we don\u2019t, we\u2019re in trouble.",
            "similar_memories": "from 2013-05-02: \n believe in the necessity of a coordinated global response when it comes to AI. Considering the issue with chatbots that lie frequently, for instance, we need rules to determine who is responsible and how we can handle this. The propagation of misinformation or biases through AI systems is a concern that needs global attention and a collective responsibility. Different policies in different regions are not in the best interest of tech companies, rather a uniform global response would serve them better. So, while challenges exist, the need for global action on AI is urgent and compelling.\n\nfrom 2013-05-02: \n believe that implementing a global, neutral and non-profit organization, involving both governments and companies, is crucial to handle the ramifications of artificial intelligence. We need coordinated efforts to build rules regarding issues such as misinformation spread by AI, its potential to perpetuate bias, and other associated challenges. It is even in the interest of tech companies to have unified policies rather than varied, nation-specific ones.\n\nfrom 2023-07-08T07:14:55.451000+00:00: \n believe global action on AI is feasible because we have seen global efforts for shared interests in the past. While some efforts may fall short, that does not make them any less essential. It is not in the interest of tech companies to have different policies everywhere, it would be beneficial to have a coordinated and global response. Failing to act on potential AI risks might lead to disasters such as machines spreading misinformation or fostering bias. Coordinated rules can help in mitigating such risks.\n\n"
        }
    },
    {
        "example": {
            "question": "There\u2019s a belief that A.I. development should be paused until we can know whether it presents undue risks. But given how new and dynamic A.I. is, how could we even know what all the undue risks are?",
            "answer": "You don\u2019t. It\u2019s part of the problem. I actually wrote my own pause letter, so to speak, with Michelle Rempel Garner. We called for a pause not on research but on deployment. The notion wasthat if you\u2019re going to deploy it on a wide scale, let\u2019s say 100 million people, then you should have to build a safety case for it, the way you do in medicine. OpenAI kind of did a version of this with their system card. They said, \u201cHere are 12 risks.\u201d But they didn\u2019t actually make a case that the benefits of people typing faster and having fun with chatbots outweighed the risks! Sam Altman has acknowledged that there\u2019s a risk of misinformation, massive cybercrime. OK, that\u2019s nice that you acknowledge it. Now the next step ought to be, before you have widespread release, let\u2019s have somebody decide: Do those risks outweigh the benefits? Or how are we even going to decide that? And at the moment, the power to release something is entirely with the companies. It has to change.",
            "similar_memories": "from 2013-05-02: \n argue that we need to view the development of A.I. in the same way we did previous infrastructures - we must have some sort of building code in place for what is acceptable and who is responsible. There needs to be dialogue and analysis to identify potential risks and despite the newness and dynamism of A.I., not doing so could lead us to greater trouble.\n\nfrom 2013-05-02: \n believe that this resembles many other domains where you have to build infrastructure to ensure safety, just like we have building codes and UL standards for electrical wiring and appliances. We might not initially like the code, but it's necessary for us to create a code that outlines what's acceptable and who's responsible when it comes to AI development.\n\nfrom 2013-05-02: \n believe a pause in A.I. development will allow us to create a code to outline what's acceptable and who is responsible, much like establishing building codes or UL standards for electrical wiring. While recognizing risks will always be a dynamic task, having fundamental guidelines will ensure we make progress safely.\n\n"
        }
    }
    ]
]