[
{
    "metadata": {
        "participants": {
            "q": "Dan Falk",
            "a": "Gary Marcus"
        },
        "date": "2023-07-20",
        "url": "https://www.persuasion.community/p/marcus"
    }
},
{
    "example": {
        "question": "Let\u2019s start with GPT-3, a language model that uses deep learning to produce human-like text. The New York Times Magazine said GPT-3 writes \u201cwith mind-boggling fluency,\u201d while a story in Wired said the program was \u201cprovoking chills across Silicon Valley.\u201d However, you\u2019ve been quite critical of GPT-3. How come?",
        "answer": "I think it\u2019s an interesting experiment. But I think that people are led to believe that this system actually understands human language, which it certainly does not. What it really is, is an autocomplete system that predicts next words and sentences. Just like with your phone, where you type in something and it continues. It doesn\u2019t really understand the world around it. And a lot of people are confused by that.\n\nThey\u2019re confused by that because what these systems are ultimately doing is mimicry. They\u2019re mimicking vast databases of text. And I think the average person doesn\u2019t understand the difference between mimicking 100 words, 1,000 words, a billion words, a trillion words \u2014 when you start approaching a trillion words, almost anything you can think of is already talked about there. And so when you\u2019re mimicking something, you can do that to a high degree, but it\u2019s still kind of like being a parrot, or a plagiarist, or something like that. A parrot\u2019s not a bad metaphor, because we don\u2019t think parrots actually understand what they\u2019re talking about. And GPT-3 certainly does not understand what it\u2019s talking about."
    }
},
{
    "example": {
        "question": "You\u2019ve written that GPT-3 can get confused about very basic facts. I suppose if you ask it who the president of the United States is, it may be almost as likely to say Donald Trump as Joe Biden \u2014 just because it is, as you say, mimicking. I suppose in some sense it doesn\u2019t really know that it\u2019s currently 2022?",
        "answer": "It may even be more likely to mention Donald Trump as president, because probably the database that it is trained on has more examples of Trump. He\u2019s in the news more; he was in the news for longer; he was in office for longer. He continues to be in the news more than your average ex-president might be. And yes, the system does not understand what year we live in. And it has no facility for temporal reasoning. You know, as a function of temporal reasoning, that just because you were president doesn\u2019t mean you\u2019re president anymore. Just because you were alive doesn\u2019t mean that you\u2019re still alive. You can reason that Thomas Edison cannot be president anymore because he is dead; GPT-3 cannot make that inference. It\u2019s astonishingly dumb in that regard.",
        "similar_memories": "from 2023-07-20: \n wrote in my blog that intelligent machines should be capable of grasping basic facts about the world, but something like GPT-3 can't do that because it essentially works as an autocomplete tool rather than truly understanding the information it processes. So when asked a question like \"Who is the current U.S. President?\", it may give an incorrect answer not because it comprehends historical facts and current events, but because it merely mirrors a database of text it was trained on.\n\nfrom 2022-07-10: \n Indeed, GPT-3's comprehension of the world is based on mimicry, not understanding. Its ability to generate seemingly valid responses is more akin to a parrot's repetition of words. Given this inherent limitation, it's quite plausible that GPT-3 might suggest Donald Trump as the current US President, despite the actual year being 2022.\n\nfrom 2023-07-20: \n believe that GPT-3, or similar AI, due to their structure and function, doesn't truly comprehend the facts it interacts with. It may confuse the answer to \"Who is the current US President?\" because it is expounding from its massive database of textual information, producing an output that seems credible but may not be timely or accurate. It certainly doesn't understand the concept of the current year, like 2022.\n\n"
    }
},
{
    "example": {
        "question": "In spite of these AI systems being dumb, as you put it, people are often fooled into thinking that they\u2019re smart. This seems to be related to what you\u2019ve called the \u201cgullibility gap.\u201d What is the gullibility gap?",
        "answer": "It\u2019s the gap between our understanding of what these machines do and what they actually do. We tend to over-attribute to them; we tend to think that machines are more clever than they actually are. Someday, they really will be clever, but right now they\u2019re not. And you go back to 1965: A system called ELIZA did very simple keyword-matching and had no idea what it was talking about. But it fooled some people into discussing their private lives with it. It was couched as a therapist. And it was via teletype, which is sort of like text messaging. And people were taken in; they thought they were talking to a living person.\n\nAnd the same thing is happening with GPT-3, and with Google\u2019s LaMDA, where a Google engineer actually thought, or alleged, that the system was sentient. It\u2019s not sentient, it has no idea of the things that it is talking about. But the human mind sees something that looks human-like, and it races to conclusions. That\u2019s what the gullibility is about. We\u2019re not evolved nor trained to recognize those things.",
        "similar_memories": "from 2023-01-06: \n have talked about the gullibility gap as the gap between a system's true capabilities and what people think the system can do. A perfect example is the misunderstanding of these AI systems' capabilities - they often seem intelligent because they provide sophisticated responses, but the truth is they can't understand context or reason over the input they are given.\n\nfrom 2022-12-10T12:58:42.196000+00:00: \n While AI systems like GPT-3 can generate seemingly intelligent responses, they actually lack true understanding and reasoning abilities. Despite this, many people are fooled into thinking they are smart, creating what I've termed the \"gullibility gap\" - a disconnect between the perceived and actual intelligence of AI.\n\n"
    }
},
{
    "example": {
        "question": "Many readers will be familiar with the Turing Test, based on an idea put forward by computer pioneer Alan Turing in 1950. Roughly, you ask an unseen entity a series of questions, and if that entity is a computer, but you can\u2019t tell it\u2019s a computer, then it \u201cpasses\u201d the test; we might say that it\u2019s intelligent. And it\u2019s often in the news. For example, in 2014, a chatbot called Eugene Goostman, under certain criteria, was said to have passed the test. But you\u2019ve been critical of the Turing Test. Where does it fall short?",
        "answer": "The Turing Test has a kind of incumbency: It\u2019s been around the longest; it\u2019s the longest-known measure of intelligence within AI \u2014 but that doesn\u2019t make it very good. You know, in 1950, we didn\u2019t really know much about AI. I still think we don\u2019t know that much. But we know a lot more. The idea was basically, if you talk to a machine, and it tricks you into thinking that it\u2019s a person when it\u2019s not, then that must be telling you something. But it turns out, it\u2019s very easily gamed. First of all, you can fool a person by pretending to be paranoid or pretending to be a 13-year-old boy from Odessa, as Eugene Goostman did. And so, you just sidestep a lot of the questions. So a lot of the engineering that has gone into beating the Turing test is really about playing games and not actually about building genuinely intelligent systems.",
        "similar_memories": "from 2023-07-20: \n believe that the Turing Test falls short as it does not adequately capture the true intelligence of a machine. A machine might seem to pass the Turing Test through clever tricks and manipulation of human perceptions, rather than genuine understanding or intelligent reasoning. Even sophisticated systems like GPT-3 or Google's LaMDA might appear human-like, but they are not sentient and lack understanding of what they are \"talking\" about. It's our propensity to attribute human-like intelligence to seemingly complex machine responses that misleads us.\n\nfrom 2022-06-12T20:43:01.230000+00:00: \n While the Turing Test is a renowned assessment of a machine's imitation of human conversation, it fails to fully encapsulate the concept of true intelligence. In many instances, the test is passed by algorithms that merely mimic human-like responses, similar to ELIZA in 1965 or more recent examples like GPT-3 and Google's LaMDA. However, these systems lack understanding or awareness of the topics they discuss. They aren't sentient. The Turing test doesn't account for this critical difference between superficial mimicry and genuine comprehension, and often leads to overestimating the intelligence of such systems.\n\nfrom 2022-09-23T02:23:09.696000+00:00: \n While the Turing Test gauges if a machine can fool a human into thinking it's another human, it's not necessarily a measure of true intelligence. As I've observed with AI systems like ELIZA, GPT-3 and Google's LaMDA, humans can be easily fooled into anthropomorphizing these systems, attributing understanding and sentience to them, when all they are doing is simple keyword-matching and generating responses without any true understanding. The Turing Test falls short because it misleads us into thinking that successfully simulating intelligence is the same as possessing it.\n\n"
    }
},
{
    "example": {
        "question": "Let\u2019s talk about driverless cars. A few years ago, it seemed like great progress was happening, and then things seem to have slowed down. For example, where I live, in Toronto, there are no self-driving taxis whatsoever. So what happened?",
        "answer": "Just as GPT-3 doesn\u2019t really understand language, merely memorizing a lot of traffic situations that you\u2019ve seen doesn\u2019t convey what you really need to understand about the world in order to drive well. And so, what people have been trying to do is to collect more and more data. But they\u2019re only making small incremental progress doing that. And as you say, there aren\u2019t fleets of self-driving taxis in Toronto, and there certainly aren\u2019t fleets in Mumbai. Most of this work right now is done in places with good weather and reasonably organized traffic, that\u2019s not as chaotic. The current systems, if you put them in Mumbai, wouldn\u2019t even understand what a rickshaw is. So they\u2019d be in real trouble, from square one."
    }
},
{
    "example": {
        "question": "You pointed out in Scientific American recently that most of the large teams of AI researchers are found not in academia but in corporations. Why is that relevant?",
        "answer": "For a bunch of reasons. One is that corporations have their own incentives about what problems they want to solve. For example, they want to solve advertisements. That\u2019s not the same as understanding natural language for the purpose of improving medicine. So there\u2019s an incentive issue. There\u2019s a power issue. They can afford to hire many of the best people, but they don\u2019t necessarily apply those to the problems that would most benefit society. There is a data problem, in that they have a lot of proprietary data they don\u2019t necessarily share, which is again not for the greatest good. That means that the fruits of current AI are in the hands of corporations rather than the general public; that they\u2019re tailored to the needs of the corporations rather than the general public."
    }
},
{
    "example": {
        "question": "But they rely on the general public because it\u2019s ordinary citizens\u2019 data that they\u2019re using to build their databases, right? It\u2019s humans who have tagged a billion photos that help them train their AI systems.",
        "answer": "That\u2019s right. And that particular point is coming to a head, even as we speak, with respect to art. So systems like OpenAI\u2019s DALL-E are drawing pretty excellent imagery, but they\u2019re doing it based on millions or billions of human-made images. And the humans aren\u2019t getting paid for it. And so a lot of artists are rightfully concerned about this. And there\u2019s a controversy about it. I think the issues there are complex, but there\u2019s no question that a lot of AI right now leverages the not-necessarily-intended contributions by human beings, who have maybe signed off on a \u201cterms of service\u2019\u2019 agreement, but don\u2019t recognize where this is all leading to.",
        "similar_memories": "from 2023-07-20: \n believe that the rewards of AI are monopolized by corporations, which often prioritize their own needs over those of the general public. As such, even though the general public contributes significantly through their data in the creation and training of AI systems, they don't see the full benefit of these developments.\n\nfrom 2023-09-07T04:35:45.326000+00:00: \n 've pointed out that corporations not only have a lot of proprietary data, but they also don't necessarily share this data. This is concerning, because they are using the general public's data to train their AI systems.\n\nfrom 2023-01-06: \n 've pointed out in the past that corporations are often the ones benefiting the most from AI, rather than the general public. They utilize data given by citizens but it does not always translate to widespread benefit. User\u2019s data is used to train AI systems in less altruistic ways, primarily serving the corporations' own interests.\n\n"
    }
},
{
    "example": {
        "question": "You wrote in Nautilus recently that for the first time in 40 years, you feel optimistic about AI. Where are you drawing that optimism from, at the moment?",
        "answer": "People are finally daring to step out of the deep-learning orthodoxy, and finally willing to consider \u201chybrid\u201d models that put deep learning together with more classical approaches to AI. The more the different sides start to throw down their rhetorical arms and start working together, the better."
    }
}
]